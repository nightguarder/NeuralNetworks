{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa93b6ea",
   "metadata": {},
   "source": [
    "# EV Charging Data Cleaning & Preparation\n",
    "\n",
    "1. This notebook standardizes timestamps, validates durations, removes invalid and extreme outliers, and saves a clean dataset for modeling.\n",
    "2. Merges datasets from multiple sources to create a comprehensive view of EV charging sessions.\n",
    "   1. Dataset 1: EV Charging Sessions_hourly (data/trondheim/Dataset_2_Hourly_EV_per_user.csv)\n",
    "   2. Dataset 2: Trondheim weather (data/trondheim/Norway_Trondheim_ExactLoc_Weather.csv)\n",
    "   3. Dataset 3: EV Charging Reports (data/trondheim/Dataset 1_EV charging reports.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609e651",
   "metadata": {},
   "source": [
    "The Logic: A user plugging in at 5 PM might stay for 2 hours (shopping) or 12 hours (overnight). The Time is the same. The difference is the Temperature (cold batteries charge slower) and the User Habit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured paths:\n",
      "  RAW_PATH: ../../data/trondheim/Dataset 1_EV charging reports.csv\n",
      "  OUT_FILE: data/ev_sessions_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RAW_PATH = '../../data/trondheim/Dataset 1_EV charging reports.csv'\n",
    "OUT_DIR = 'data'\n",
    "OUT_FILE = os.path.join(OUT_DIR, 'ev_sessions_clean.csv')\n",
    "\n",
    "print('Configured paths:')\n",
    "print('  RAW_PATH:', RAW_PATH)\n",
    "print('  OUT_FILE:', OUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1322f8c",
   "metadata": {},
   "source": [
    "Load Dataset 1_EV charging reports.csv.\n",
    "Load Norway_Trondheim_ExactLoc_Weather.csv.\n",
    "Merge: You must merge them on the Start_plugin timestamp.\n",
    "Code Hint: Round the session start time to the nearest hour, then merge with the weather dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b74a28",
   "metadata": {},
   "source": [
    "## Quick sanity checks (run first)\n",
    "\n",
    "These cells load the cleaned dataset, report duration statistics and candidate target class balances, and run a short merge diagnostic with the weather file.\n",
    "\n",
    "Paste and run these cells before further merging or modeling — they will flag any class-balance or merge-key issues immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff159923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: load cleaned dataset and inspect duration / candidate targets\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "clean_path = os.path.join('data','ev_sessions_clean.csv')\n",
    "print('Checking cleaned dataset at:', clean_path)\n",
    "if not os.path.exists(clean_path):\n",
    "    print('File not found:', clean_path)\n",
    "else:\n",
    "    # parse common datetime columns if present\n",
    "    try:\n",
    "        df = pd.read_csv(clean_path, parse_dates=['Start_plugin_dt','End_plugout_dt'], dayfirst=True)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(clean_path)\n",
    "    print('shape:', df.shape)\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df.head())\n",
    "    except Exception:\n",
    "        print(df.head().to_string())\n",
    "\n",
    "    # Ensure we have a Duration_hours column (some notebooks use Duration_hours_calc)\n",
    "    if 'Duration_hours' not in df.columns:\n",
    "        if 'Duration_hours_calc' in df.columns:\n",
    "            df['Duration_hours'] = df['Duration_hours_calc']\n",
    "            print('Using Duration_hours_calc -> Duration_hours')\n",
    "        else:\n",
    "            print('WARNING: no Duration_hours column found. Check cleaning notebook outputs.')\n",
    "\n",
    "    if 'Duration_hours' in df.columns:\n",
    "        print('\\nDuration_hours summary (selected percentiles):')\n",
    "        print(df['Duration_hours'].describe(percentiles=[0.5,0.75,0.9,0.95,0.99]))\n",
    "\n",
    "        # Candidate thresholds: 12h, 24h, 48h\n",
    "        for t in [12, 24, 48]:\n",
    "            prop = (df['Duration_hours'] < t).mean()\n",
    "            cnt = int((df['Duration_hours'] < t).sum())\n",
    "            print(f\"<{t}h : {prop:.4f} ({cnt} samples)\" )\n",
    "\n",
    "        # Create a 24h binary target and show distribution (normalize)\n",
    "        df['is_short_24'] = (df['Duration_hours'] < 24).astype(int)\n",
    "        print('\\nTarget (is_short_24) distribution (normalize):')\n",
    "        print(df['is_short_24'].value_counts(normalize=True))\n",
    "        print('Absolute counts:')\n",
    "        print(df['is_short_24'].value_counts())\n",
    "\n",
    "        short_prop = df['is_short_24'].mean()\n",
    "        if short_prop > 0.90:\n",
    "            print('WARNING: short class proportion > 90% — accuracy will be misleading. Use precision/recall on LONG class.')\n",
    "\n",
    "        p90 = df['Duration_hours'].quantile(0.90)\n",
    "        print('90th percentile duration:', p90)\n",
    "\n",
    "        # Save small sample for downstream merge diagnostics\n",
    "        df_sample = df[['Start_plugin_dt','Duration_hours']].copy() if 'Start_plugin_dt' in df.columns else df.head(100).copy()\n",
    "        # Keep df in notebook namespace for following cells\n",
    "        globals()['df'] = df\n",
    "        globals()['df_sample'] = df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge diagnostics: attempt to locate weather datetime column and report match rate\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "weather_path = os.path.join('..','..','data','trondheim','Norway_Trondheim_ExactLoc_Weather.csv')\n",
    "print('Checking weather file at:', weather_path)\n",
    "if not os.path.exists(weather_path):\n",
    "    print('Weather file not found at expected path. Adjust path and re-run.')\n",
    "else:\n",
    "    # load with minimal parsing, we'll try to find datetime column(s)\n",
    "    weather = pd.read_csv(weather_path)\n",
    "    print('Weather columns:', list(weather.columns)[:30])\n",
    "    # heuristics to find datetime-like column\n",
    "    dt_candidates = [c for c in weather.columns if any(k in c.lower() for k in ['date','time','dt','timestamp','hour'])]\n",
    "    if len(dt_candidates) == 0:\n",
    "        # try parsing first object column\n",
    "        for c in weather.columns:\n",
    "            if weather[c].dtype == object:\n",
    "                parsed = pd.to_datetime(weather[c], errors='coerce', dayfirst=True)\n",
    "                if parsed.notna().sum() > 0:\n",
    "                    weather['weather_dt_candidate'] = parsed\n",
    "                    dt_candidates = ['weather_dt_candidate']\n",
    "                    break\n",
    "    if len(dt_candidates) == 0:\n",
    "        print('No datetime-like column auto-detected in weather file — open it to choose the correct column for merging.')\n",
    "    else:\n",
    "        wc = dt_candidates[0]\n",
    "        print('Using weather datetime column:', wc)\n",
    "        weather['weather_dt'] = pd.to_datetime(weather[wc], errors='coerce', dayfirst=True)\n",
    "        weather['weather_hour'] = weather['weather_dt'].dt.round('H')\n",
    "        # Ensure df and Start_plugin_dt exist from previous cell\n",
    "        if 'df' not in globals():\n",
    "            print('df not available in namespace — run the previous sanity-check cell first')\n",
    "        else:\n",
    "            if 'Start_plugin_dt' not in df.columns:\n",
    "                print('df missing Start_plugin_dt — ensure cleaning notebook produced this column')\n",
    "            else:\n",
    "                df['start_hour'] = df['Start_plugin_dt'].dt.round('H')\n",
    "                merged = df.merge(weather[['weather_hour']], left_on='start_hour', right_on='weather_hour', how='left', indicator=True)\n",
    "                print('Weather merge indicator counts:')\n",
    "                print(merged['_merge'].value_counts())\n",
    "                matched = merged['_merge'].eq('both').mean()\n",
    "                print(f'Percent matched to weather: {matched:.2%}')\n",
    "                # keep merged sample for inspection\n",
    "                globals()['merged_sample'] = merged.head(20)\n",
    "                try:\n",
    "                    from IPython.display import display\n",
    "                    display(merged.head())\n",
    "                except Exception:\n",
    "                    print(merged.head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5a3a5",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- If the 24h target is extremely imbalanced (e.g. >90% short), consider alternative targets (12h, quantile-based) or treat as rare-event problem.\n",
    "- If weather merge match rate is low, re-check rounding strategy (floor vs round) and timezone handling.\n",
    "- Once these checks pass, apply the existing feature-engineering cells from the cleaning notebook to this merged dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c6b5a",
   "metadata": {},
   "source": [
    "## Raw-dataset scaffold: load & diagnostics\n",
    "\n",
    "This cell loads the three raw files (reports, hourly per-user, weather), prints shapes and sample rows, auto-detects datetime-like columns and common join keys, and computes simple overlap diagnostics. Run this before cleaning so we can confirm the raw inputs and choose merge keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8266a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaffold diagnostics: load raw files and inspect\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base = os.path.join('..','..','data','trondheim')\n",
    "paths = {\n",
    "    'reports': os.path.join(base, 'Dataset 1_EV charging reports.csv'),\n",
    "    'hourly': os.path.join(base, 'Dataset_2_Hourly_EV_per_user.csv'),\n",
    "    'weather': os.path.join(base, 'Norway_Trondheim_ExactLoc_Weather.csv')\n",
    "}\n",
    "\n",
    "loaded = {}\n",
    "for name, p in paths.items():\n",
    "    print(f\"\\n-> {name}: {p}\")\n",
    "    if not os.path.exists(p):\n",
    "        print('   MISSING')\n",
    "        loaded[name] = None\n",
    "        continue\n",
    "    try:\n",
    "        # try light parsing (don't force too many datetimes yet)\n",
    "        df_raw = pd.read_csv(p, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print('   Failed to read:', e)\n",
    "        loaded[name] = None\n",
    "        continue\n",
    "    loaded[name] = df_raw\n",
    "    print('   shape:', df_raw.shape)\n",
    "    print('   columns (first 20):', list(df_raw.columns)[:20])\n",
    "    display(df_raw.head(3))\n",
    "\n",
    "# helper to auto-detect datetime-like columns\n",
    "def detect_datetime_cols(df):\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(k in lc for k in ['date','time','start','end','dt','timestamp','hour']):\n",
    "            candidates.append(c)\n",
    "    # try parsing a few object columns if none found\n",
    "    if len(candidates) == 0:\n",
    "        for c in df.select_dtypes(include=['object']).columns[:10]:\n",
    "            parsed = pd.to_datetime(df[c], errors='coerce', dayfirst=True)\n",
    "            if parsed.notna().sum() > 0.01 * len(df):\n",
    "                candidates.append(c)\n",
    "    return candidates\n",
    "\n",
    "# detect datetime cols\n",
    "for name, df_raw in loaded.items():\n",
    "    if df_raw is None:\n",
    "        continue\n",
    "    print(f\"\\n{name} datetime candidates:\")\n",
    "    dtc = detect_datetime_cols(df_raw)\n",
    "    print(dtc)\n",
    "\n",
    "# detect possible key columns (station/user)\n",
    "key_names = ['Garage_ID','GarageID','StationID','LocationID','Shared_ID','SharedId','SharedID','UserID','user_id']\n",
    "for name, df_raw in loaded.items():\n",
    "    if df_raw is None:\n",
    "        continue\n",
    "    found = [k for k in key_names if k in df_raw.columns]\n",
    "    print(f\"\\n{name} found key columns: {found}\")\n",
    "    for k in found:\n",
    "        print(f\"  unique {k}: {df_raw[k].nunique()} values, top 5:\\n\", df_raw[k].value_counts().head(5))\n",
    "\n",
    "# basic hourly distribution for reports if Start_plugin available\n",
    "if loaded.get('reports') is not None:\n",
    "    rep = loaded['reports'].copy()\n",
    "    # try common start col names\n",
    "    start_cols = [c for c in rep.columns if 'start' in c.lower() or 'plugin' in c.lower()]\n",
    "    print('\\nreports candidate start cols:', start_cols[:5])\n",
    "    if len(start_cols) > 0:\n",
    "        col = start_cols[0]\n",
    "        rep['_parsed_start'] = pd.to_datetime(rep[col], dayfirst=True, errors='coerce')\n",
    "        print('Parsed start nulls:', rep['_parsed_start'].isna().sum())\n",
    "        if rep['_parsed_start'].notna().any():\n",
    "            rep['start_hour'] = rep['_parsed_start'].dt.round('H')\n",
    "            print('Sessions per start_hour (sample):')\n",
    "            print(rep['start_hour'].dt.floor('D').value_counts().sort_index().head())\n",
    "\n",
    "# quick merge overlap example: reports <> weather on rounded hour\n",
    "if loaded.get('reports') is not None and loaded.get('weather') is not None:\n",
    "    rpt = loaded['reports'].copy()\n",
    "    wth = loaded['weather'].copy()\n",
    "    # detect and parse start in reports\n",
    "    rpt_start_candidates = detect_datetime_cols(rpt)\n",
    "    wth_start_candidates = detect_datetime_cols(wth)\n",
    "    print('\\nUsing rpt_start_candidates:', rpt_start_candidates)\n",
    "    print('Using wth_start_candidates:', wth_start_candidates)\n",
    "    if len(rpt_start_candidates) > 0 and len(wth_start_candidates) > 0:\n",
    "        rpt['_s'] = pd.to_datetime(rpt[rpt_start_candidates[0]], dayfirst=True, errors='coerce')\n",
    "        wth['_w'] = pd.to_datetime(wth[wth_start_candidates[0]], dayfirst=True, errors='coerce')\n",
    "        rpt['start_hour'] = rpt['_s'].dt.round('H')\n",
    "        wth['weather_hour'] = wth['_w'].dt.round('H')\n",
    "        merged_example = rpt.merge(wth[['weather_hour']].drop_duplicates(), left_on='start_hour', right_on='weather_hour', how='left', indicator=True)\n",
    "        print('Weather merge indicator counts (example):')\n",
    "        print(merged_example['_merge'].value_counts())\n",
    "        print('Percent matched:', merged_example['_merge'].eq('both').mean())\n",
    "    else:\n",
    "        print('Cannot auto-detect suitable datetime columns for merge example — inspect the candidate lists above.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
