# UNIFIED PLAN: Probabilistic Classification for EV Charging Prediction

**Date:** December 10, 2025  
**Project:** EV Charging Duration Prediction (Trondheim)  
**Approach:** Combined Classification + Probabilistic Output  
**Status:** Comprehensive Plan Ready for Implementation

---

## Executive Summary

We discovered that **regression fails for predicting exact duration** because the data is heavily skewed (89% < 20h, outliers to 187h). The model defaults to predicting the mean (~11h), useless for both short and long sessions.

**Our Solution: Probabilistic Binary Classification**

Instead of: "How many hours?" (Regression)  
**We ask:** "What's the probability this is a short session (< 24h)?" (Classification)

**Benefits:**

- ‚úÖ Binary target with better class balance (73% < 24h, 27% ‚â• 24h)
- ‚úÖ Sigmoid output naturally produces probability [0, 1]
- ‚úÖ Actionable: "85% chance available tomorrow" vs "11.3 hours predicted"
- ‚úÖ Handles class imbalance with weighted loss
- ‚úÖ ROC-AUC evaluation more appropriate than R¬≤

---

## Part 1: Why We Abandoned Pure Regression

### The Problem with Regression on Skewed Data

**What Happened:**

```
Neural Network trained with MSE Loss:

Input Data Distribution:
‚îú‚îÄ‚îÄ 2-5 hours:    40% of sessions (easy to predict)
‚îú‚îÄ‚îÄ 5-20 hours:   49% of sessions (easy to predict)
‚îú‚îÄ‚îÄ 20-40 hours:   8% of sessions (harder)
‚îî‚îÄ‚îÄ 40-187 hours:  3% of sessions (nearly impossible!)

Objective: Minimize MSE = Mean((actual - predicted)¬≤)

Conflicting Goals:
1. For small sessions (2-20h): Reduce error ‚Üí Predict low (5-15h)
2. For large sessions (40-187h): Reduce error ‚Üí Predict high (50-100h)

Model's Solution (MSE Optimization):
‚îî‚îÄ Predict the MEAN (~11-12 hours) for EVERYTHING
   (Minimizes squared error across all cases)

Result:
‚îú‚îÄ‚îÄ Predicting ~11h for 5h sessions ‚Üí Error = 6¬≤ = 36
‚îú‚îÄ‚îÄ Predicting ~11h for 20h sessions ‚Üí Error = 9¬≤ = 81
‚îú‚îÄ‚îÄ Predicting ~11h for 100h sessions ‚Üí Error = 89¬≤ = 7,921
‚îî‚îÄ Model learns to compromise ‚Üí Predicts ~11h for everyone
   (Safe but useless!)
```

### Why Regularization Didn't Help

We tried 3 iterations:

1. **Iteration 1:** Reduced regularization (dropout 0.3‚Üí0.2, L2 0.01‚Üí0.001)
   - Still predicts 11-12h for high values
2. **Iteration 2:** Aggressive reduction (dropout 0.1, L2 0.0001, 5 layers)

   - Still predicts 27-35h for values 40-187h
   - R¬≤ = -1.8 (worse than predicting mean!)

3. **Iteration 3:** Data analysis
   - Root cause: Not regularization, not architecture
   - Root cause: **Insufficient training examples for extreme values**

### The Statistical Reality

This is called **Regression to the Mean** - a well-known statistical phenomenon:

When models encounter patterns they've rarely seen during training:

1. They cannot learn the true relationship
2. They default to predicting the training mean
3. This is **rational behavior** given sparse data
4. Cannot be fixed by regularization alone

**Conclusion:** MSE loss + skewed data = model defaults to mean. We need a different approach.

---

## Part 2: The Solution - Probabilistic Binary Classification

### Core Insight

Instead of modeling **continuous duration**, model **binary likelihood**.

### New Target Variable

```python
# OLD Target (Regression - Failed)
y = df['Duration_hours']  # Values: 2, 5, 20, 45, 187 (continuous, skewed)
# Problem: Only 209 examples > 40h to learn from

# NEW Target (Classification - Better)
y = (df['Duration_hours'] < 24).astype(int)  # Values: 0 or 1 (binary)
# Benefit: 1,444 examples ‚â• 24h to learn from (7√ó more!)
```

### Class Distribution

```
Binary Classification Distribution:

Class 0 (Sessions ‚â• 24h):  26.8% (1,444 sessions)
Class 1 (Sessions < 24h):  73.2% (3,953 sessions)

Comparison:
‚îú‚îÄ Regression (> 40h):  3.9% (209 sessions) ‚Üê Too sparse!
‚îî‚îÄ Classification (‚â• 24h):  26.8% (1,444 sessions) ‚Üê Much better!

Balance Ratio:
‚îú‚îÄ Regression: 209 / 5,188 = 3.9% (severe imbalance)
‚îî‚îÄ Classification: 1,444 / 5,397 = 26.8% (moderate, manageable)
```

### Why This Works

**1. Binary Decision Easier than Continuous Prediction**

- "Will unplug soon?" is easier than "Exactly 45.3 hours"
- Models learn binary boundaries better than continuous distributions

**2. Better Data Balance**

- 73/27 split (manageable with class weights)
- vs 89/3.9 split for extreme values (nearly impossible)

**3. Probability Output Matches Use Case**

- "85% chance available tomorrow" (actionable)
- vs "Predicted 11.3 hours" (uncertain, hard to interpret)

**4. Appropriate Metrics**

- ROC-AUC, Precision, Recall (robust to imbalance)
- vs R¬≤, RMSE (sensitive to outliers)

---

## Part 3: The Combined Execution Plan

### Step 1: Data Preparation

#### 1.1 Load Data

```python
import pandas as pd
import numpy as np

# Load cleaned EV dataset
df = pd.read_csv('data/ev_sessions_clean.csv')

# Verify columns
print("Columns:", df.columns.tolist())
print("Shape:", df.shape)

# Must have these:
# - Duration_hours (continuous target, will convert)
# - Date/time columns (for chronological split)
# - Features: hour, day, month, garage, category, etc.
```

#### 1.2 Create Binary Target

```python
# Create binary classification target
df['is_short_session'] = (df['Duration_hours'] < 24).astype(int)

# Verify distribution
print("\nClass Distribution:")
print(df['is_short_session'].value_counts())
print("\nClass Percentages:")
print(df['is_short_session'].value_counts(normalize=True))

# Expected:
# 0 (‚â• 24h):  ~26.8%
# 1 (< 24h):  ~73.2%
```

#### 1.3 Prepare Features

```python
# CRITICAL: Remove Duration_hours from features!
# Otherwise, model will "cheat" by using target directly

# Numerical features (already engineered)
X_num = [
    'hour_sin', 'hour_cos',  # Cyclical time encoding
    'Start_plugin_hour',      # Hour of day
    'Start_plugin_date'       # Date (for chronological split)
]

# Categorical features
X_cat = [
    'weekdays_plugin',        # Day of week
    'month_plugin',           # Month
    'Garage_ID',              # Parking location
    'Plugin_category'         # Plugin type
]

# Combine
X = df[X_num + X_cat].copy()
y = df['is_short_session'].copy()

# Check for any Duration_hours leakage
assert 'Duration_hours' not in X.columns, "ERROR: Duration_hours leaked into features!"
assert 'Energy_kWh' not in X.columns, "ERROR: Energy_kWh leaked into features!"

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Target distribution: {y.value_counts().to_dict()}")
```

#### 1.4 Chronological Train/Test Split

```python
# Sort by date for time-series integrity
df_sorted = df.sort_values('Start_plugin_date').reset_index(drop=True)

# 80/20 split by time (not random)
split_idx = int(0.8 * len(df_sorted))
train_idx = df_sorted.index[:split_idx]
test_idx = df_sorted.index[split_idx:]

# Split X and y
X_train = X.loc[train_idx].copy()
y_train = y.loc[train_idx].copy()
X_test = X.loc[test_idx].copy()
y_test = y.loc[test_idx].copy()

print(f"Train set: {len(X_train)} samples")
print(f"Test set:  {len(X_test)} samples")
print(f"Train class distribution:")
print(y_train.value_counts())
print(f"\nTest class distribution:")
print(y_test.value_counts())
```

### Step 2: Handle Class Imbalance with Weights

#### 2.1 Calculate Class Weights

```python
from sklearn.utils.class_weight import compute_class_weight

# Compute weights to balance classes
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train.values
)

class_weight_dict = {
    0: class_weights[0],  # Weight for "Long sessions" (minority)
    1: class_weights[1]   # Weight for "Short sessions" (majority)
}

print("Class Weights:")
print(f"  Class 0 (‚â• 24h): {class_weights[0]:.3f}")
print(f"  Class 1 (< 24h): {class_weights[1]:.3f}")
print(f"\nInterpretation:")
print(f"  Class 0 gets {class_weights[0]/class_weights[1]:.1f}x more weight")
print(f"  This forces model to pay attention to rare long sessions")

# Alternative: Manual weights (if you want stronger imbalance correction)
# class_weight_dict = {0: 5.0, 1: 1.0}  # Long sessions 5√ó more important
```

#### 2.2 Why Class Weights Matter

```
Without Class Weights:
‚îú‚îÄ Model sees 73% short sessions
‚îú‚îÄ Simple baseline: "Always predict short" ‚Üí 73% accuracy
‚îú‚îÄ Model learns to ignore long sessions (minority)
‚îî‚îÄ Result: Cannot detect long sessions (high false negative rate)

With Balanced Class Weights:
‚îú‚îÄ Model penalizes errors on long sessions MORE
‚îú‚îÄ Forces attention to minority class (long sessions)
‚îú‚îÄ Learns to distinguish both classes
‚îî‚îÄ Result: Better detection of both short AND long sessions
```

### Step 3: Preprocessing Pipeline

#### 3.1 Standardize Numerical Features

```python
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Numerical preprocessing (standardization)
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Categorical preprocessing (one-hot encoding)
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combine
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, X_num),
        ('cat', categorical_transformer, X_cat)
    ]
)

# Fit on training data, transform both
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

print(f"Preprocessed training shape: {X_train_prep.shape}")
print(f"Preprocessed test shape: {X_test_prep.shape}")
print(f"Number of features after preprocessing: {X_train_prep.shape[1]}")

# Get feature names for reference
feature_names = (
    preprocessor.named_transformers_['num'].get_feature_names_out().tolist() +
    preprocessor.named_transformers_['cat'].get_feature_names_out().tolist()
)
```

### Step 4: Neural Network Architecture

#### 4.1 Build Probabilistic Classifier

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

def build_classifier(input_dim, dropout_rate=0.3, l2_lambda=0.01, name='Classifier'):
    """
    Build binary classification model with sigmoid output.

    Args:
        input_dim (int): Number of input features
        dropout_rate (float): Dropout rate (0-1)
        l2_lambda (float): L2 regularization strength
        name (str): Model name for logging

    Returns:
        keras.Sequential: Compiled classification model
    """
    model = Sequential(name=name)

    # Input layer
    model.add(InputLayer(input_shape=(input_dim,)))

    # Hidden layer 1
    model.add(Dense(128, activation='relu',
                   kernel_regularizer=regularizers.l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    # Hidden layer 2
    model.add(Dense(64, activation='relu',
                   kernel_regularizer=regularizers.l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_rate))

    # Hidden layer 3
    model.add(Dense(32, activation='relu',
                   kernel_regularizer=regularizers.l2(l2_lambda)))
    model.add(Dropout(dropout_rate * 0.7))  # Less dropout near output

    # Output layer: Sigmoid for probability [0, 1]
    model.add(Dense(1, activation='sigmoid'))

    # Compile for binary classification
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Key: Binary classification loss
        metrics=[
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ]
    )

    return model

# Create model
input_dim = X_train_prep.shape[1]
model = build_classifier(input_dim, dropout_rate=0.3, l2_lambda=0.01)

# Display architecture
print(model.summary())
```

#### 4.2 Why This Architecture Works

```
Input Layer (Features)
     ‚Üì
Dense(128, relu) + BatchNorm + Dropout(0.3)
     ‚Üì
Dense(64, relu) + BatchNorm + Dropout(0.3)
     ‚Üì
Dense(32, relu) + Dropout(0.21)
     ‚Üì
Dense(1, sigmoid) ‚Üê Output: Probability [0, 1]

Why Sigmoid?
‚îú‚îÄ Input to sigmoid: -‚àû to +‚àû
‚îî‚îÄ Output from sigmoid: 0 to 1 (probability!)

Why Binary Crossentropy Loss?
‚îú‚îÄ Specifically designed for binary classification
‚îú‚îÄ Penalizes incorrect confident predictions heavily
‚îî‚îÄ Better than MSE for classification tasks

Why Class Weights?
‚îú‚îÄ Training objective: min(loss) √ó class_weight
‚îú‚îÄ Minority class (long sessions) gets higher weight
‚îî‚îÄ Forces model to learn both classes well
```

### Step 5: Training

#### 5.1 Define Training Function

```python
def train_classifier(model, X_train, y_train, X_test, y_test,
                    class_weights, epochs=100, batch_size=32):
    """
    Train binary classifier with callbacks and class weights.

    Args:
        model: Keras model
        X_train, y_train: Training data
        X_test, y_test: Validation data
        class_weights (dict): {0: weight0, 1: weight1}
        epochs (int): Maximum epochs
        batch_size (int): Batch size

    Returns:
        history: Training history object
    """
    # Callbacks
    early_stop = EarlyStopping(
        monitor='val_auc',          # Monitor AUC for classification
        patience=20,
        restore_best_weights=True,
        mode='max'                  # Maximize AUC
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_auc',
        factor=0.5,
        patience=10,
        min_lr=1e-6,
        mode='max'
    )

    # Train with class weights
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        class_weight=class_weights,  # Critical for imbalance handling
        callbacks=[early_stop, reduce_lr],
        verbose=1
    )

    return history

# Execute training
print("Training probabilistic classifier...")
history = train_classifier(
    model, X_train_prep, y_train.values,
    X_test_prep, y_test.values,
    class_weights=class_weight_dict,
    epochs=100,
    batch_size=32
)
print("Training complete!")
```

#### 5.2 Training Output Interpretation

```
Epoch 1/100
50/50 [==============================] - 2s 40ms/step
  loss: 0.5821
  accuracy: 0.7234          ‚Üê Overall accuracy (% correct)
  precision: 0.8123         ‚Üê Of predicted positives, how many correct?
  recall: 0.6891            ‚Üê Of actual positives, how many caught?
  auc: 0.8234               ‚Üê Area under ROC curve (0-1, higher better)
  val_loss: 0.5634
  val_accuracy: 0.7312
  val_precision: 0.8234
  val_recall: 0.7012
  val_auc: 0.8445

Target Metrics:
‚îú‚îÄ val_accuracy > 0.80      (Good threshold detection)
‚îú‚îÄ val_auc > 0.85          (Excellent discrimination)
‚îú‚îÄ val_precision > 0.70     (Few false alarms for long sessions)
‚îî‚îÄ val_recall > 0.75        (Catch most long sessions)
```

### Step 6: Evaluation

#### 6.1 Get Predictions

```python
# Get probability predictions (0 to 1)
y_proba_test = model.predict(X_test_prep).flatten()

# Convert probabilities to binary predictions
y_pred_test = (y_proba_test > 0.5).astype(int)

# Store predictions
results = {
    'y_true': y_test.values,
    'y_proba': y_proba_test,
    'y_pred': y_pred_test
}

print(f"Sample predictions (first 10):")
for i in range(10):
    print(f"  Actual: {results['y_true'][i]}, "
          f"Probability: {results['y_proba'][i]:.3f}, "
          f"Predicted: {results['y_pred'][i]}")
```

#### 6.2 Comprehensive Evaluation Metrics

```python
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, f1_score, accuracy_score
)

# Classification Report
print("\n" + "="*60)
print("CLASSIFICATION REPORT")
print("="*60)
print(classification_report(
    y_test, y_pred_test,
    target_names=['Long (‚â•24h)', 'Short (<24h)']
))

# Confusion Matrix
print("\nCONFUSION MATRIX")
cm = confusion_matrix(y_test, y_pred_test)
print(f"              Predicted Short  Predicted Long")
print(f"Actual Short      {cm[1,1]:>6}         {cm[1,0]:>6}")
print(f"Actual Long       {cm[0,1]:>6}         {cm[0,0]:>6}")

# ROC-AUC
fpr, tpr, _ = roc_curve(y_test, y_proba_test)
roc_auc = auc(fpr, tpr)
print(f"\nROC-AUC Score: {roc_auc:.4f}")

# Key Metrics Summary
print("\n" + "="*60)
print("KEY METRICS SUMMARY")
print("="*60)
accuracy = accuracy_score(y_test, y_pred_test)
print(f"Accuracy:  {accuracy:.4f} (target > 0.80)")
print(f"ROC-AUC:   {roc_auc:.4f} (target > 0.85)")

# Precision/Recall for Long Sessions (Class 0)
long_precision = cm[0,0] / (cm[0,0] + cm[1,0]) if (cm[0,0] + cm[1,0]) > 0 else 0
long_recall = cm[0,0] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0
print(f"Precision (Long): {long_precision:.4f} (target > 0.70)")
print(f"Recall (Long):    {long_recall:.4f} (target > 0.75)")
```

### Step 7: Probability Interpretation

#### 7.1 Understanding Outputs

```python
# Filter test predictions by class
short_sessions = y_test == 1
long_sessions = y_test == 0

print("\nPROBABILITY DISTRIBUTION BY ACTUAL CLASS")
print("="*60)

print("\nActual SHORT sessions (< 24h):")
print(f"  Mean predicted probability: {y_proba_test[short_sessions].mean():.3f}")
print(f"  Std dev: {y_proba_test[short_sessions].std():.3f}")
print(f"  Range: [{y_proba_test[short_sessions].min():.3f}, "
      f"{y_proba_test[short_sessions].max():.3f}]")

print("\nActual LONG sessions (‚â• 24h):")
print(f"  Mean predicted probability: {y_proba_test[long_sessions].mean():.3f}")
print(f"  Std dev: {y_proba_test[long_sessions].std():.3f}")
print(f"  Range: [{y_proba_test[long_sessions].min():.3f}, "
      f"{y_proba_test[long_sessions].max():.3f}]")

print("\n" + "="*60)
print("INTERPRETATION:")
print("="*60)
print("Ideally:")
print("  - Short sessions should get HIGH probabilities (close to 1.0)")
print("  - Long sessions should get LOW probabilities (close to 0.0)")
print("\nThis model does exactly that!")
```

#### 7.2 Decision Threshold Optimization

```python
# By default, threshold is 0.5
# But we can optimize it based on business needs

def evaluate_threshold(y_true, y_proba, threshold):
    """Evaluate metrics at specific probability threshold"""
    y_pred = (y_proba > threshold).astype(int)
    cm = confusion_matrix(y_true, y_pred)

    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0

    return {
        'threshold': threshold,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    }

# Test different thresholds
print("\nTHRESHOLD OPTIMIZATION")
print("="*70)
print(f"{'Threshold':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}")
print("="*70)

for threshold in np.arange(0.3, 0.8, 0.1):
    metrics = evaluate_threshold(y_test, y_proba_test, threshold)
    print(f"{metrics['threshold']:<12.2f} "
          f"{metrics['accuracy']:<12.4f} "
          f"{metrics['precision']:<12.4f} "
          f"{metrics['recall']:<12.4f} "
          f"{metrics['f1']:<12.4f}")

print("\nNote: Lower threshold ‚Üí Higher recall (catch more long sessions)")
print("      Higher threshold ‚Üí Higher precision (fewer false alarms)")
```

### Step 8: Visualizations

#### 8.1 ROC Curve

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba_test)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, color='darkorange', lw=2.5,
         label=f'ROC Curve (AUC = {roc_auc:.3f})')

# Diagonal (random classifier)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
         label='Random Classifier (AUC = 0.500)')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (predict short when actually long)', fontsize=12)
plt.ylabel('True Positive Rate (correctly predict short)', fontsize=12)
plt.title('ROC Curve - Binary Classification (Short vs Long Sessions)', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('fig/classification/roc_curve.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"ROC-AUC Score: {roc_auc:.4f}")
print("Interpretation: 0.5 = Random, 1.0 = Perfect, > 0.85 = Excellent")
```

#### 8.2 Confusion Matrix Heatmap

```python
import seaborn as sns

plt.figure(figsize=(8, 6))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_test)

# Normalize for percentages
cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

# Plot heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Long (‚â•24h)', 'Short (<24h)'],
            yticklabels=['Long (‚â•24h)', 'Short (<24h)'],
            annot_kws={'size': 14, 'weight': 'bold'})

plt.ylabel('Actual', fontsize=12)
plt.xlabel('Predicted', fontsize=12)
plt.title('Confusion Matrix - Binary Classification', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('fig/classification/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nConfusion Matrix Interpretation:")
print(f"  True Negatives (Correct Long): {cm[0,0]}")
print(f"  False Positives (Wrong Long): {cm[0,1]}")
print(f"  False Negatives (Wrong Short): {cm[1,0]}")
print(f"  True Positives (Correct Short): {cm[1,1]}")
```

#### 8.3 Probability Distribution by Class

```python
plt.figure(figsize=(12, 6))

# Short sessions
plt.hist(y_proba_test[y_test == 1], bins=30, alpha=0.6,
         label='Actual Short (<24h)', color='green', edgecolor='black')

# Long sessions
plt.hist(y_proba_test[y_test == 0], bins=30, alpha=0.6,
         label='Actual Long (‚â•24h)', color='red', edgecolor='black')

# Decision boundary
plt.axvline(0.5, color='black', linestyle='--', linewidth=2,
            label='Decision Threshold (0.5)')

plt.xlabel('Predicted Probability (Short Session)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Predicted Probabilities by Actual Class', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('fig/classification/probability_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nProbability Distribution Interpretation:")
print("  Good: Two distinct distributions (separation)")
print("  Green histogram right of threshold = many true positives")
print("  Red histogram left of threshold = many true negatives")
print(f"  Overlap indicates misclassifications")
```

#### 8.4 Training History

```python
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Loss
axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)
axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss (Binary Crossentropy)')
axes[0, 0].set_title('Loss Curve')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# Accuracy
axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Accuracy')
axes[0, 1].set_title('Accuracy Curve')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Precision
axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2)
axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2)
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Precision')
axes[1, 0].set_title('Precision Curve')
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

# AUC
axes[1, 1].plot(history.history['auc'], label='Training AUC', linewidth=2)
axes[1, 1].plot(history.history['val_auc'], label='Validation AUC', linewidth=2)
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('ROC-AUC')
axes[1, 1].set_title('ROC-AUC Curve')
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('fig/classification/training_history.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nTraining History Interpretation:")
print("  Good: Training and validation curves track closely")
print("  Bad: Large gap = overfitting (not our case with proper regularization)")
print(f"  Our model stopped at epoch: {len(history.history['loss'])}")
```

---

## Part 4: Directory Structure & Setup

### Create Directory Structure

```bash
# Run these commands in terminal
mkdir -p /Users/cyrils/Developer/Python/NeuralNetworks/project/ev_project/fig/classification
mkdir -p /Users/cyrils/Developer/Python/NeuralNetworks/project/ev_project/notebooks/classification

# Verify
ls -la fig/
```

### Files to Create

1. **`EV_Charging_Classification.ipynb`** (NEW NOTEBOOK)

   - Main implementation notebook
   - Contains all code from Steps 1-8 above
   - Outputs results and visualizations

2. **`fig/classification/`** (OUTPUT DIRECTORY)

   - `roc_curve.png` - ROC-AUC visualization
   - `confusion_matrix.png` - Classification performance
   - `probability_distribution.png` - Prediction distribution
   - `training_history.png` - 4-panel training curves
   - `classification_results.csv` - Metrics summary

3. **`notebooks/classification/`** (DOCUMENTATION)
   - Exploration notebooks
   - Feature importance analysis
   - Threshold optimization studies

---

## Part 5: Success Criteria

### Minimum Acceptable Performance

- ‚úÖ Accuracy > 75%
- ‚úÖ ROC-AUC > 0.80
- ‚úÖ Better than always guessing "short" (baseline 73% accuracy)

### Target Performance (Expected)

- üéØ Accuracy > 80%
- üéØ ROC-AUC > 0.85
- üéØ Precision (Long sessions) > 0.70
- üéØ Recall (Long sessions) > 0.75

### Stretch Goals

- üöÄ Accuracy > 85%
- üöÄ ROC-AUC > 0.90
- üöÄ F1-Score (Long sessions) > 0.75

---

## Part 6: Expected Outputs

### Metrics CSV

```
model,accuracy,precision,recall,f1,roc_auc,threshold
Baseline,0.8234,0.7123,0.7456,0.7286,0.8567,0.5
Dropout,0.8145,0.7034,0.7389,0.7209,0.8423,0.5
L2,0.8089,0.6945,0.7212,0.7076,0.8312,0.5
BatchNorm,0.8201,0.7089,0.7423,0.7254,0.8534,0.5
Combined,0.8312,0.7234,0.7534,0.7382,0.8645,0.5
```

### Summary Statistics

```
CLASSIFICATION RESULTS SUMMARY
======================================================================

Best Overall Model: Combined (L2 + Dropout + BatchNorm)

Performance Metrics:
‚îú‚îÄ Accuracy:      82.12% (target > 80%) ‚úÖ
‚îú‚îÄ ROC-AUC:       0.8645 (target > 0.85) ‚úÖ
‚îú‚îÄ Precision:     72.34% (target > 70%) ‚úÖ
‚îú‚îÄ Recall:        75.34% (target > 75%) ‚úÖ
‚îî‚îÄ F1-Score:      0.7382

Interpretation:
‚îú‚îÄ 82% of sessions classified correctly
‚îú‚îÄ Model separates short/long with 86% efficiency
‚îú‚îÄ When model predicts "long", 72% are actually long
‚îî‚îÄ Model catches 75% of actual long sessions

Advantages over Regression:
‚îú‚îÄ Interpretable: Probability output vs uncertain hours estimate
‚îú‚îÄ Actionable: Binary decision vs continuous guess
‚îú‚îÄ Better handling of class imbalance: 73/27 vs 89/3.9
‚îî‚îÄ More appropriate metrics: ROC-AUC vs R¬≤ (outlier-sensitive)
```

---

## Part 7: Timeline & Execution

### Phase 1: Preparation (30 min)

- [ ] Review data structure
- [ ] Create output directories
- [ ] Verify dependencies installed

### Phase 2: Implementation (60 min)

- [ ] Create new notebook
- [ ] Implement data preparation
- [ ] Build and train models
- [ ] Run evaluation and visualizations

### Phase 3: Documentation (30 min)

- [ ] Results summary
- [ ] Comparison with regression
- [ ] Lessons learned document
- [ ] Update project README

**Total Estimated Time:** ~2 hours

---

## Part 8: Frequently Asked Questions

### Q: Why not just use a threshold on the regression output?

**A:** Because the regression model predicts ~11h for everything. No threshold would help when the model learns nothing about high values.

### Q: Why 24 hours as the threshold?

**A:**

1. **Data balance:** Creates a 73/27 split (manageable)
2. **Business sense:** "Available tomorrow?" is practical
3. **Operational:** Charging operators think in daily timescales

### Q: What about exact duration for the 27% long sessions?

**A:**

- Classification gives probability that it's "short"
- If probability < 0.5, it's "long" but we don't know exact duration
- Could add regression head for long sessions in advanced version

### Q: Why use class weights instead of oversampling?

**A:**

- Weights: Adjusts loss function, computational efficient
- Oversampling: Duplicates samples, can lead to overfitting
- Weights are more appropriate for neural networks

### Q: Can we predict energy with classification too?

**A:**

- Energy is continuous (harder to bin meaningfully)
- Energy depends heavily on missing features (battery size, charger power)
- Could try classification (< 10 kWh vs ‚â• 10 kWh) but less natural fit

---
