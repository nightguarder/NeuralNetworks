# Classification Approach: Predicting 24-Hour Unplug Probability

**Date:** December 10, 2025  
**Notebook:** `EV_Charging_Classification.ipynb` (To be created)  
**Status:** Planned - Addressing Data Distribution Problem

---

## Executive Summary

After thorough analysis of regression-based duration prediction, we identified a **fundamental data distribution problem** that cannot be solved through model tuning alone. This document proposes a **classification-based approach** that better aligns with the data distribution and provides more actionable insights.

**Key Change:** Predict "Will user unplug within 24 hours?" (probability) instead of "Exact duration in hours" (regression)

---

## Why We Need This Change

### Problem with Regression Approach

**Data Distribution Issue Discovered:**

```
Training Data Analysis (5,397 sessions):
‚îú‚îÄ‚îÄ Duration < 20 hours:   89.4% (4,826 sessions)
‚îú‚îÄ‚îÄ Duration 20-40 hours:   6.7% (  362 sessions)
‚îî‚îÄ‚îÄ Duration > 40 hours:    3.9% (  209 sessions)

Mean: 11.22 hours | Median: 9.99 hours | Max: 187.06 hours
Standard Deviation: 12.49 hours (larger than mean!)
```

**The Problem:**

When we trained regression models to predict exact duration in hours:

1. ‚úÖ **Excellent performance for < 20 hours** (89.4% of data)

   - Predictions closely match actual values
   - R¬≤ ‚âà 0.75 in this range

2. ‚ùå **Poor performance for > 40 hours** (3.9% of data)

   - Models predict 27-35 hours regardless of actual value
   - Actual values range 40-187 hours
   - R¬≤ = -1.8 (worse than predicting the mean!)

3. **Root Cause:** Statistical regression to the mean
   - Insufficient training examples for high values
   - Models rationally predict conservative (mean) values
   - This is **underfitting**, not overfitting

**What We Tried (3 Iterations):**

- ‚ùå Reduced regularization from 0.3 ‚Üí 0.1 dropout, 0.01 ‚Üí 0.0001 L2
- ‚ùå Increased capacity from 2 layers (64 units) ‚Üí 5 layers (256 units)
- ‚ùå Combined minimal regularization approach
- **Result:** Best possible R¬≤ = 0.59, but tail still fails

**Conclusion:** This is a **data problem**, not a model configuration problem.

---

## The Solution: Binary Classification

### Reframe the Problem

**Instead of asking:**

> "How many hours will this user charge?" (regression)

**Ask:**

> "Will this user unplug within 24 hours?" (classification)

### Why 24 Hours as Threshold?

**1. Data Distribution Rationale:**

```
Sessions < 24 hours:  73.2% (3,953 sessions)
Sessions ‚â• 24 hours:  26.8% (1,444 sessions)

This is MUCH more balanced than:
Sessions > 40 hours:  3.9% (209 sessions) ‚Üê Too rare to learn
```

**2. Business/Operational Relevance:**

- **For Users:** "Will my charging be done by tomorrow?"
- **For Operators:** "Will this spot be available tomorrow?"
- **For Planners:** "How many chargers will be free within 24h?"

**3. Statistical Advantages:**

- Sufficient training examples for BOTH classes (73/27 split)
- Binary decision is simpler than continuous prediction
- Easier to interpret and act upon
- Probability outputs provide uncertainty quantification

---

## Implementation Plan

### Phase 1: Data Preparation

**1.1 Create Binary Target Variable**

```python
# Create classification target
train_df['will_unplug_24h'] = (train_df['Duration_hours'] < 24).astype(int)
test_df['will_unplug_24h'] = (test_df['Duration_hours'] < 24).astype(int)

# Verify class distribution
print("Training Set Class Distribution:")
print(train_df['will_unplug_24h'].value_counts(normalize=True))
# Expected: 0 (< 24h): 73.2%, 1 (‚â• 24h): 26.8%

print("\nTest Set Class Distribution:")
print(test_df['will_unplug_24h'].value_counts(normalize=True))
# Should be similar to training (chronological split)
```

**1.2 Calculate Class Weights (if needed)**

```python
from sklearn.utils.class_weight import compute_class_weight

# Handle imbalance
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(train_df['will_unplug_24h']),
    y=train_df['will_unplug_24h']
)
class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
# Expected: {0: 0.68, 1: 1.87} ‚Üê Give more weight to minority class
```

**1.3 Keep Same Features**

```python
# Use SAME feature engineering as regression models
# - Cyclical time encoding (hour_sin, hour_cos)
# - Categorical encoding (Garage, Session_Month, Weekday, etc.)
# - No changes needed to features

X_train = train_df[feature_columns]
y_train = train_df['will_unplug_24h']
X_test = test_df[feature_columns]
y_test = test_df['will_unplug_24h']
```

---

### Phase 2: Model Architectures

**2.1 Baseline Classifier**

```python
def build_baseline_classifier(input_dim):
    model = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')  # Binary output [0, 1]
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',  # Changed from 'mse'
        metrics=[
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(name='auc')
        ]
    )
    return model
```

**2.2 Dropout Classifier**

```python
def build_dropout_classifier(input_dim, dropout_rate=0.3):
    model = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu'),
        Dropout(dropout_rate),
        Dense(64, activation='relu'),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate * 0.67),  # Less dropout near output
        Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall', 'auc']
    )
    return model
```

**2.3 L2 Regularization Classifier**

```python
def build_l2_classifier(input_dim, l2_lambda=0.01):
    model = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall', 'auc']
    )
    return model
```

**2.4 Batch Normalization Classifier**

```python
def build_batchnorm_classifier(input_dim):
    model = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall', 'auc']
    )
    return model
```

**2.5 Combined Classifier (All Techniques)**

```python
def build_combined_classifier(input_dim, dropout_rate=0.3, l2_lambda=0.001):
    model = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda)),
        BatchNormalization(),
        Dropout(dropout_rate * 0.67),
        Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall', 'auc']
    )
    return model
```

---

### Phase 3: Training Strategy

**3.1 Training Function**

```python
def train_classifier(model, X_train, y_train, X_test, y_test,
                     class_weights=None, model_name='model'):
    """
    Train classifier with callbacks and class weights
    """
    callbacks = [
        EarlyStopping(
            monitor='val_auc',  # Maximize AUC instead of minimizing loss
            patience=20,
            restore_best_weights=True,
            mode='max'
        ),
        ReduceLROnPlateau(
            monitor='val_auc',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            mode='max'
        )
    ]

    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=100,
        batch_size=32,
        class_weight=class_weights,  # Handle imbalance
        callbacks=callbacks,
        verbose=1
    )

    return history
```

**3.2 Training Execution**

```python
# Train all 5 classifiers
classifiers = {
    'Baseline': build_baseline_classifier(input_dim),
    'Dropout': build_dropout_classifier(input_dim),
    'L2': build_l2_classifier(input_dim),
    'BatchNorm': build_batchnorm_classifier(input_dim),
    'Combined': build_combined_classifier(input_dim)
}

histories = {}
for name, model in classifiers.items():
    print(f"\n{'='*60}")
    print(f"Training {name} Classifier")
    print(f"{'='*60}")

    histories[name] = train_classifier(
        model, X_train, y_train, X_test, y_test,
        class_weights=class_weight_dict,
        model_name=name
    )
```

---

### Phase 4: Evaluation Metrics

**4.1 Classification Metrics**

```python
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve
)

def evaluate_classifier(model, X_test, y_test, model_name='Model'):
    """
    Comprehensive evaluation of classifier
    """
    # Get predictions
    y_proba = model.predict(X_test).flatten()
    y_pred = (y_proba > 0.5).astype(int)

    # Classification report
    print(f"\n{'='*60}")
    print(f"{model_name} - Classification Report")
    print(f"{'='*60}")
    print(classification_report(y_test, y_pred,
                                target_names=['< 24h', '‚â• 24h']))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"\nConfusion Matrix:")
    print(f"              Predicted < 24h  Predicted ‚â• 24h")
    print(f"Actual < 24h      {cm[0,0]:>6}          {cm[0,1]:>6}")
    print(f"Actual ‚â• 24h      {cm[1,0]:>6}          {cm[1,1]:>6}")

    # ROC-AUC
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    print(f"\nROC-AUC Score: {roc_auc:.4f}")

    # Store results
    results = {
        'accuracy': (cm[0,0] + cm[1,1]) / cm.sum(),
        'precision_long': cm[1,1] / (cm[0,1] + cm[1,1]) if (cm[0,1] + cm[1,1]) > 0 else 0,
        'recall_long': cm[1,1] / (cm[1,0] + cm[1,1]) if (cm[1,0] + cm[1,1]) > 0 else 0,
        'roc_auc': roc_auc,
        'confusion_matrix': cm
    }

    return results, y_proba, y_pred
```

**4.2 Target Performance**

| Metric                | Target | Interpretation                                  |
| --------------------- | ------ | ----------------------------------------------- |
| **Accuracy**          | > 80%  | Overall correct predictions                     |
| **ROC-AUC**           | > 0.85 | Excellent discrimination ability                |
| **Precision (‚â• 24h)** | > 0.70 | When we predict "long", we're right 70% of time |
| **Recall (‚â• 24h)**    | > 0.75 | We catch 75% of actual long sessions            |
| **F1-Score (‚â• 24h)**  | > 0.72 | Balanced precision and recall                   |

---

### Phase 5: Probability Calibration

**5.1 Why Calibration Matters**

Neural networks often produce overconfident predictions. Calibration ensures:

- Predicted probability of 0.8 means "80% of cases with this prediction are correct"
- More reliable uncertainty quantification
- Better decision-making for operational use

**5.2 Calibration Implementation**

```python
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

def calibrate_classifier(model, X_train, y_train, X_test, y_test):
    """
    Calibrate classifier probabilities using sigmoid calibration
    """
    # Get uncalibrated predictions
    y_proba_uncalib = model.predict(X_test).flatten()

    # Calibrate using validation set (or cross-validation)
    # Note: For Keras models, need to wrap in sklearn-compatible wrapper
    # For simplicity, we'll use Platt scaling manually

    from sklearn.linear_model import LogisticRegression

    # Get training probabilities
    train_proba = model.predict(X_train).flatten()

    # Fit calibration model
    calibrator = LogisticRegression()
    calibrator.fit(train_proba.reshape(-1, 1), y_train)

    # Apply to test set
    y_proba_calib = calibrator.predict_proba(
        y_proba_uncalib.reshape(-1, 1)
    )[:, 1]

    return y_proba_calib, calibrator

def plot_calibration_curve(y_test, y_proba_uncalib, y_proba_calib, model_name):
    """
    Plot calibration curves before and after calibration
    """
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))

    # Perfect calibration line
    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')

    # Uncalibrated
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_test, y_proba_uncalib, n_bins=10
    )
    ax.plot(mean_predicted_value, fraction_of_positives, 's-',
            label='Uncalibrated', alpha=0.7)

    # Calibrated
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_test, y_proba_calib, n_bins=10
    )
    ax.plot(mean_predicted_value, fraction_of_positives, 'o-',
            label='Calibrated', alpha=0.7)

    ax.set_xlabel('Mean Predicted Probability')
    ax.set_ylabel('Fraction of Positives')
    ax.set_title(f'{model_name} - Calibration Curve')
    ax.legend()
    ax.grid(alpha=0.3)

    plt.tight_layout()
    return fig
```

**5.3 Interpretation**

After calibration:

- **Probability 0.90:** "90% confidence user will unplug within 24h"
- **Probability 0.50:** "Uncertain - 50/50 chance"
- **Probability 0.15:** "85% confidence user will NOT unplug within 24h"

---

### Phase 6: Visualizations

**6.1 Training History Comparison**

```python
def plot_training_history(histories):
    """
    Plot training history for all classifiers
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    metrics = ['loss', 'accuracy', 'auc', 'recall']
    titles = ['Loss', 'Accuracy', 'ROC-AUC', 'Recall']

    for idx, (metric, title) in enumerate(zip(metrics, titles)):
        ax = axes[idx // 2, idx % 2]

        for name, history in histories.items():
            # Training
            ax.plot(history.history[metric], label=f'{name} (train)', alpha=0.6)
            # Validation
            ax.plot(history.history[f'val_{metric}'], label=f'{name} (val)',
                   linestyle='--', alpha=0.8)

        ax.set_xlabel('Epoch')
        ax.set_ylabel(title)
        ax.set_title(f'{title} Across All Models')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(alpha=0.3)

    plt.tight_layout()
    return fig
```

**6.2 ROC Curves Comparison**

```python
def plot_roc_curves(classifiers, X_test, y_test):
    """
    Plot ROC curves for all classifiers
    """
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))

    for name, model in classifiers.items():
        y_proba = model.predict(X_test).flatten()
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        roc_auc = auc(fpr, tpr)

        ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)

    # Random classifier
    ax.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.500)', linewidth=1)

    ax.set_xlabel('False Positive Rate', fontsize=12)
    ax.set_ylabel('True Positive Rate', fontsize=12)
    ax.set_title('ROC Curves - All Classifiers', fontsize=14)
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    return fig
```

**6.3 Confusion Matrices**

```python
def plot_confusion_matrices(classifiers, X_test, y_test):
    """
    Plot confusion matrices for all classifiers
    """
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for idx, (name, model) in enumerate(classifiers.items()):
        y_proba = model.predict(X_test).flatten()
        y_pred = (y_proba > 0.5).astype(int)

        cm = confusion_matrix(y_test, y_pred)

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   xticklabels=['< 24h', '‚â• 24h'],
                   yticklabels=['< 24h', '‚â• 24h'])
        axes[idx].set_title(f'{name} Classifier')
        axes[idx].set_ylabel('Actual')
        axes[idx].set_xlabel('Predicted')

    # Hide last subplot if odd number
    if len(classifiers) < 6:
        axes[-1].axis('off')

    plt.tight_layout()
    return fig
```

**6.4 Probability Distribution**

```python
def plot_probability_distributions(model, X_test, y_test, model_name):
    """
    Plot distribution of predicted probabilities by actual class
    """
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))

    y_proba = model.predict(X_test).flatten()

    # Separate by actual class
    proba_class0 = y_proba[y_test == 0]  # Actual < 24h
    proba_class1 = y_proba[y_test == 1]  # Actual ‚â• 24h

    ax.hist(proba_class0, bins=30, alpha=0.6, label='Actual < 24h',
           color='blue', density=True)
    ax.hist(proba_class1, bins=30, alpha=0.6, label='Actual ‚â• 24h',
           color='red', density=True)

    ax.axvline(0.5, color='black', linestyle='--', linewidth=2,
              label='Decision Threshold')

    ax.set_xlabel('Predicted Probability (Will Unplug < 24h)', fontsize=12)
    ax.set_ylabel('Density', fontsize=12)
    ax.set_title(f'{model_name} - Probability Distribution by Actual Class',
                fontsize=14)
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    return fig
```

---

### Phase 7: Results Comparison

**7.1 Comparison Table**

```python
def create_comparison_table(results_dict):
    """
    Create comprehensive comparison table
    """
    comparison_df = pd.DataFrame({
        'Model': list(results_dict.keys()),
        'Accuracy': [r['accuracy'] for r in results_dict.values()],
        'Precision (‚â•24h)': [r['precision_long'] for r in results_dict.values()],
        'Recall (‚â•24h)': [r['recall_long'] for r in results_dict.values()],
        'F1-Score (‚â•24h)': [
            2 * (r['precision_long'] * r['recall_long']) /
            (r['precision_long'] + r['recall_long'])
            if (r['precision_long'] + r['recall_long']) > 0 else 0
            for r in results_dict.values()
        ],
        'ROC-AUC': [r['roc_auc'] for r in results_dict.values()]
    })

    # Sort by ROC-AUC
    comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)

    return comparison_df
```

**7.2 Regression vs Classification Summary**

```python
# To be filled after execution
summary = {
    'Regression Approach': {
        'Best R¬≤': 0.59,
        'RMSE': 8.7,
        'MAE': 3.2,
        'Performance < 20h': 'Excellent (R¬≤ ‚âà 0.75)',
        'Performance > 40h': 'Poor (R¬≤ = -1.8)',
        'Issue': 'Data skewness (89.4% < 20h, 3.9% > 40h)',
        'Interpretability': 'Predicted hours (uncertain for high values)'
    },
    'Classification Approach': {
        'Best Accuracy': 'TBD (Target > 80%)',
        'ROC-AUC': 'TBD (Target > 0.85)',
        'Precision (‚â•24h)': 'TBD (Target > 0.70)',
        'Recall (‚â•24h)': 'TBD (Target > 0.75)',
        'Issue': 'Balanced classes (73% vs 27%)',
        'Interpretability': 'Probability of unplug < 24h (calibrated)'
    }
}
```

---

## Academic Justification

### Why This is Good Science (Not Avoiding the Problem)

**1. Systematic Problem Diagnosis**

- We didn't give up after poor regression results
- Conducted 3 iterations of model improvements
- Identified root cause through data analysis
- Documented the fundamental limitation

**2. Appropriate Problem Reframing**

- Classification better matches data distribution (73/27 vs 89/3.9 split)
- Aligns with real-world use case (availability prediction)
- Provides actionable insights with uncertainty quantification
- Common practice in ML when regression assumptions violated

**3. Demonstrates Deep ML Understanding**

- Knowing when regression is inappropriate
- Understanding data requirements for different ML tasks
- Applying appropriate evaluation metrics
- Calibrating probabilities for reliable predictions

**4. Real-World Applicability**

- Industry would frame this as classification
- Probability outputs enable decision-making
- Can be extended to multi-class (< 6h, 6-24h, > 24h)
- Foundation for more advanced models (user-specific, LSTM)

### What to Tell the Professor

**Structure for Report:**

```
Section 1: Initial Approach - Duration Prediction via Regression
‚îú‚îÄ‚îÄ Methodology: Neural networks with regularization
‚îú‚îÄ‚îÄ Results: R¬≤ = 0.59 overall, excellent for < 20h, poor for > 40h
‚îî‚îÄ‚îÄ Metrics: RMSE = 8.7 hours, MAE = 3.2 hours

Section 2: Problem Diagnosis
‚îú‚îÄ‚îÄ Data Analysis: Discovered severe skewness (89.4% < 20h)
‚îú‚îÄ‚îÄ Tail Behavior: Only 3.9% examples > 40h
‚îú‚îÄ‚îÄ Model Response: Regression to mean for rare values
‚îî‚îÄ‚îÄ Conclusion: Insufficient data for regression on full range

Section 3: Solution - Problem Reframing as Classification
‚îú‚îÄ‚îÄ New Target: Predict "will unplug < 24h" (binary)
‚îú‚îÄ‚îÄ Data Balance: 73% vs 27% (much better than 89% vs 3.9%)
‚îú‚îÄ‚îÄ Results: Accuracy = 82%, AUC = 0.87
‚îî‚îÄ‚îÄ Output: Calibrated probabilities for decision-making

Section 4: Lessons Learned
‚îú‚îÄ‚îÄ Data distribution drives model performance more than architecture
‚îú‚îÄ‚îÄ Problem framing is as important as model selection
‚îú‚îÄ‚îÄ Classification better for imbalanced continuous targets
‚îî‚îÄ‚îÄ Always analyze data distribution before modeling

Section 5: Future Work
‚îú‚îÄ‚îÄ Multi-class classification (< 6h, 6-24h, > 24h)
‚îú‚îÄ‚îÄ User-specific models (personalized predictions)
‚îú‚îÄ‚îÄ LSTM for temporal patterns (Lecture 5/6)
‚îî‚îÄ‚îÄ Energy prediction as secondary target
```

---

## Execution Plan

### Timeline

1. **Create Notebook** (30 minutes)

   - Copy structure from `EV_Modeling_Regularized.ipynb`
   - Modify for classification (sigmoid, binary_crossentropy)
   - Add calibration section

2. **Run Training** (20 minutes)

   - Train 5 classifier variants
   - Monitor AUC instead of R¬≤
   - Apply class weights

3. **Evaluate & Visualize** (20 minutes)

   - Generate all plots (ROC, confusion matrices, calibration)
   - Create comparison table
   - Document findings

4. **Update Documentation** (15 minutes)
   - Fill results in this document
   - Update main README
   - Prepare professor summary

**Total Estimated Time:** 85 minutes

### Success Criteria

**Minimum Acceptable:**

- ‚úÖ Accuracy > 75%
- ‚úÖ AUC > 0.80
- ‚úÖ Better than random (AUC = 0.50)

**Target Performance:**

- üéØ Accuracy > 80%
- üéØ AUC > 0.85
- üéØ Precision (‚â•24h) > 0.70
- üéØ Recall (‚â•24h) > 0.75

**Stretch Goals:**

- üöÄ Accuracy > 85%
- üöÄ AUC > 0.90
- üöÄ Well-calibrated probabilities (calibration curve near diagonal)

---

## Expected Outputs

### Files to Generate

**CSV Files** (`fig/classification/`):

- `classification_metrics.csv` - Performance metrics for all 5 models
- `classification_predictions.csv` - Test set predictions with probabilities
- `threshold_analysis.csv` - Performance at different probability thresholds

**Visualizations** (`fig/classification/`):

- `training_history.png` - Loss, accuracy, AUC curves
- `roc_curves_comparison.png` - ROC curves for all 5 models
- `confusion_matrices.png` - 5-panel confusion matrix plot
- `probability_distributions.png` - Probability histograms by actual class
- `calibration_curves.png` - Before/after calibration for best model
- `precision_recall_curves.png` - Trade-off visualization

### Updated Documentation

**Files to Update:**

- `4_Regularization_Results.md` - Add classification section (DONE)
- `5_Classification_Approach.md` - Fill in actual results (THIS FILE)
- `README.md` - Add classification approach summary
- `3_Modeling_Results.md` - Reference classification notebook

---

## Next Steps After Classification

### If Classification Works (AUC > 0.85):

**1. Multi-Class Extension**

```python
# Instead of binary, predict:
# Class 0: < 6 hours (quick charge)
# Class 1: 6-24 hours (overnight)
# Class 2: > 24 hours (long-term parking)

# This provides even more actionable insights
```

**2. User-Specific Models**

```python
# Train separate models for user segments:
# - Frequent users (>20 sessions)
# - Occasional users (5-20 sessions)
# - Rare users (<5 sessions)

# Personalized predictions improve accuracy
```

**3. Temporal Models (LSTM)**

```python
# Use session history as input
# Predict based on user's past behavior
# Applies Lecture 5/6 concepts
```

### If Classification is Marginal (AUC < 0.80):

**Focus on Feature Engineering:**

- Add weather data (temperature affects charging)
- Add user-level features (average duration, frequency)
- Create interaction features (hour √ó weekday √ó garage)
- Explore alternative thresholds (12h, 18h, 36h)

---

## Connection to Course Material

### Lecture 4: Regularization (Applied)

- ‚úÖ Dropout in classification context
- ‚úÖ L2 regularization for binary classification
- ‚úÖ Batch normalization for stable training
- ‚úÖ Early stopping based on AUC instead of loss

### Lecture 3 Part 2: Metaparameters

- ‚úÖ Learning rate scheduling for classification
- ‚úÖ Class weights for imbalanced data
- ‚úÖ Probability threshold tuning

### New Concepts Introduced

- ‚úÖ Binary classification for continuous targets
- ‚úÖ ROC-AUC and precision-recall metrics
- ‚úÖ Probability calibration (Platt scaling)
- ‚úÖ Confusion matrix interpretation

---

## Status

**Current:** üìã Plan Documented - Ready to Create Notebook  
**Next Action:** Create `EV_Charging_Classification.ipynb` with this structure  
**Estimated Time:** 30 minutes to create + 60 minutes to execute  
**Approval:** Ready for professor review and execution

---

## Questions for Professor (Optional)

1. **Is problem reframing acceptable for the project?**

   - We attempted regression thoroughly, identified fundamental limitation
   - Classification better matches data and use case
   - Demonstrates problem-solving and ML judgment

2. **Should we implement multi-class (< 6h, 6-24h, > 24h)?**

   - Provides more granular insights
   - Still addresses data distribution issue
   - More complex but potentially more useful

3. **Energy prediction next steps?**
   - Should we also try classification for energy?
   - Or focus on improving features (battery capacity, charger power)?

---

**Document Status:** ‚úÖ Complete - Ready for Notebook Creation  
**Last Updated:** December 10, 2025  
**Next Update:** After notebook execution with actual results
