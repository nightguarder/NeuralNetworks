{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544682f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports (safety cell)\n",
    "# This cell ensures common names like `np`, `pd`, `plt`, `sns` are defined\n",
    "# so running individual later cells or static analysis won't report NameError.\n",
    "try:\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    np = None\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    plt = None\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdeba2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9132740",
   "metadata": {},
   "source": [
    "# EV Charging — Dataset readiness, EDA and prototype models\n",
    "\n",
    "This notebook loads the residential EV charging dataset (local copy if available), runs exploratory data analysis (EDA), implements simple rule-of-thumb checks to decide if there is enough data to train a neural network, builds a small baseline model, and trains a tiny PyTorch MLP as a feasibility test.\n",
    "\n",
    "Goals:\n",
    "- Inspect dataset size, span and per-user distribution.\n",
    "- Provide automated heuristics for \n",
    " for a small NN.\n",
    "- Implement a linear baseline and a minimal PyTorch MLP.\n",
    "- Summarize findings and next steps for the class project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4595a",
   "metadata": {},
   "source": [
    "## 1) Imports and environment checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb059eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_torch = True device = mps\n"
     ]
    }
   ],
   "source": [
    "# Standard data stack\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ML libs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# Try to import torch (optional)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    has_torch = True\n",
    "    device = torch.device(\"cpu\")\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "    except Exception:\n",
    "        # some torch builds may not expose mps attribute cleanly\n",
    "        device = torch.device(\"cpu\")\n",
    "except Exception as e:\n",
    "    has_torch = False\n",
    "    device = None\n",
    "\n",
    "print('has_torch =', has_torch, 'device =', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c03823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed cwd to repo root: /Users/cyrils/Developer/Python/NeuralNetworks\n",
      "Inserted repo_root to sys.path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ensure notebook runs from the repository root so relative paths work\n",
    "cwd = Path.cwd()\n",
    "repo_root = None\n",
    "markers = ['README.md', '.git', 'pyproject.toml']\n",
    "\n",
    "# 1) Search upwards from cwd\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    if any((parent / m).exists() for m in markers):\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "# 2) If not found, search immediate child directories (this handles kernels started one level above the repo)\n",
    "if repo_root is None:\n",
    "    for child in cwd.iterdir():\n",
    "        try:\n",
    "            if child.is_dir() and any((child / m).exists() for m in markers):\n",
    "                repo_root = child\n",
    "                break\n",
    "        except PermissionError:\n",
    "            continue\n",
    "\n",
    "# 3) As a last resort, do a shallow two-level search (child's children)\n",
    "if repo_root is None:\n",
    "    for child in cwd.iterdir():\n",
    "        try:\n",
    "            if not child.is_dir():\n",
    "                continue\n",
    "            for g in child.iterdir():\n",
    "                try:\n",
    "                    if g.is_dir() and any((g / m).exists() for m in markers):\n",
    "                        repo_root = g\n",
    "                        break\n",
    "                except PermissionError:\n",
    "                    continue\n",
    "            if repo_root is not None:\n",
    "                break\n",
    "        except PermissionError:\n",
    "            continue\n",
    "\n",
    "# If still not found, default to cwd\n",
    "if repo_root is None:\n",
    "    repo_root = cwd\n",
    "\n",
    "# Change cwd if necessary\n",
    "if repo_root != cwd:\n",
    "    try:\n",
    "        os.chdir(repo_root)\n",
    "        print('Changed cwd to repo root:', Path.cwd())\n",
    "    except Exception as e:\n",
    "        # don't crash the notebook; print diagnostic and continue from current cwd\n",
    "        print('Failed to change cwd to repo_root:', repo_root, 'error:', e)\n",
    "else:\n",
    "    print('Repo root appears to be current working directory:', repo_root)\n",
    "\n",
    "# Optionally add repo root to sys.path so notebook imports can find project modules\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "    print('Inserted repo_root to sys.path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e003b9",
   "metadata": {},
   "source": [
    "## 2) Load dataset (tries a few local paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bf66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/trondheim/Dataset_2_Hourly_EV_per_user.csv\n",
      "Loaded — rows: 88156 columns: ['date_from', 'date_to', 'User_ID', 'session_ID', 'Synthetic_3_6kW', 'Synthetic_7_2kW', 'Flex_3_6kW', 'Flex_7_2kW']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>User_ID</th>\n",
       "      <th>session_ID</th>\n",
       "      <th>Synthetic_3_6kW</th>\n",
       "      <th>Synthetic_7_2kW</th>\n",
       "      <th>Flex_3_6kW</th>\n",
       "      <th>Flex_7_2kW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-21 10:00:00</td>\n",
       "      <td>2018-12-21 11:00:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-21 10:00:00</td>\n",
       "      <td>2018-12-21 11:00:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-21 11:00:00</td>\n",
       "      <td>2018-12-21 12:00:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-21 12:00:00</td>\n",
       "      <td>2018-12-21 13:00:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-21 13:00:00</td>\n",
       "      <td>2018-12-21 14:00:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from             date_to User_ID  session_ID  \\\n",
       "0 2018-12-21 10:00:00 2018-12-21 11:00:00  AdO3-4         1.0   \n",
       "1 2018-12-21 10:00:00 2018-12-21 11:00:00  AdO3-4         2.0   \n",
       "2 2018-12-21 11:00:00 2018-12-21 12:00:00  AdO3-4         3.0   \n",
       "3 2018-12-21 12:00:00 2018-12-21 13:00:00  AdO3-4         3.0   \n",
       "4 2018-12-21 13:00:00 2018-12-21 14:00:00  AdO3-4         3.0   \n",
       "\n",
       "   Synthetic_3_6kW  Synthetic_7_2kW  Flex_3_6kW  Flex_7_2kW  \n",
       "0             0.30             0.30         NaN       0.060  \n",
       "1             0.87             0.87         NaN       0.114  \n",
       "2             1.62             3.24         NaN         NaN  \n",
       "3             3.60             7.20         NaN         NaN  \n",
       "4             3.60             7.20         NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column types:\n",
      "date_from          datetime64[ns]\n",
      "date_to            datetime64[ns]\n",
      "User_ID                    object\n",
      "session_ID                float64\n",
      "Synthetic_3_6kW           float64\n",
      "Synthetic_7_2kW           float64\n",
      "Flex_3_6kW                float64\n",
      "Flex_7_2kW                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Robust dataset loader: try DATASET_PATH env var first, then project-relative candidates.\n",
    "from pathlib import Path\n",
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "candidates = [\n",
    "    Path('data/trondheim/Dataset_2_Hourly_EV_per_user.csv'),\n",
    "    Path('data/trondheim/Dataset_2_Hourly_EV_per_user_sample.csv'),\n",
    "]\n",
    "\n",
    "found = None\n",
    "# prefer explicit env var\n",
    "dp = os.getenv('DATASET_PATH')\n",
    "if dp:\n",
    "    p = Path(dp)\n",
    "    if p.exists():\n",
    "        found = p\n",
    "\n",
    "if found is None:\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            found = p\n",
    "            break\n",
    "    # glob fallback\n",
    "    if found is None:\n",
    "        import glob\n",
    "        matches = list(Path('data/trondheim').glob('*.csv')) if Path('data/trondheim').exists() else []\n",
    "        if matches:\n",
    "            found = matches[0]\n",
    "\n",
    "\n",
    "def robust_read_ev(path: Path) -> pd.DataFrame:\n",
    "    text = path.read_text(encoding='utf-8', errors='replace')\n",
    "    # detect delimiter from header\n",
    "    first_line = text.splitlines()[0]\n",
    "    if ';' in first_line:\n",
    "        sep=';'\n",
    "    elif '\\t' in first_line:\n",
    "        sep='\\t'\n",
    "    else:\n",
    "        sep=','\n",
    "\n",
    "    # First-pass read with detected separator using python engine (more permissive)\n",
    "    try:\n",
    "        df = pd.read_csv(io.StringIO(text), sep=sep, engine='python')\n",
    "    except Exception:\n",
    "        df = pd.read_csv(io.StringIO(text), sep=sep, engine='python', on_bad_lines='skip')\n",
    "\n",
    "    # If we ended up with a single column that still contains separators, reparse explicitly\n",
    "    if df.shape[1] == 1:\n",
    "        sample = df.iloc[0,0] if len(df) else ''\n",
    "        if isinstance(sample, str) and (';' in sample or '\\t' in sample):\n",
    "            df = pd.read_csv(io.StringIO(text), sep=';', engine='python', on_bad_lines='skip')\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "    # Clean numeric-looking columns: remove tabs, non-breaking spaces, stray spaces, convert comma decimals\n",
    "    def clean_numeric_series(s: pd.Series) -> pd.Series:\n",
    "        s = s.fillna('').astype(str)\n",
    "        s = s.str.replace('\\t', '', regex=True)\n",
    "        s = s.str.replace('\\xa0', '', regex=False)\n",
    "        s = s.str.replace(' ', '', regex=False)\n",
    "        s = s.str.replace('\\u00A0', '', regex=False)\n",
    "        s = s.str.replace(',', '.', regex=False)\n",
    "        # remove any stray semicolons or letters like 'NA'\n",
    "        s = s.str.replace(';', '', regex=False)\n",
    "        s = s.replace({'NA': '', 'na': '', 'NaN': ''})\n",
    "        return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "    # Heuristic: energy columns contain words like 'Synthetic' or 'Flex' or 'kW' or 'kwh'\n",
    "    energy_cols = [c for c in df.columns if isinstance(c, str) and (re.search(r'Synthetic|Flex|kW|kwh', c, re.I))]\n",
    "    for c in energy_cols:\n",
    "        df[c] = clean_numeric_series(df[c])\n",
    "\n",
    "    # parse date_from/date_to if present\n",
    "    if 'date_from' in df.columns:\n",
    "        df['date_from'] = pd.to_datetime(df['date_from'].astype(str).str.strip(), dayfirst=True, errors='coerce', format='%d.%m.%Y %H:%M')\n",
    "    if 'date_to' in df.columns:\n",
    "        df['date_to'] = pd.to_datetime(df['date_to'].astype(str).str.strip(), dayfirst=True, errors='coerce', format='%d.%m.%Y %H:%M')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load\n",
    "if found is None:\n",
    "    print('No local file found. Please download the Kaggle dataset and place the CSV in data/trondheim/ or set DATASET_PATH.')\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    print('Loading', found)\n",
    "    try:\n",
    "        df = robust_read_ev(found)\n",
    "        print('Loaded — rows:', len(df), 'columns:', list(df.columns))\n",
    "    except Exception as e:\n",
    "        print('Error reading file robustly:', type(e).__name__, e)\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "# quick peek if loaded\n",
    "if not df.empty:\n",
    "    display(df.head())\n",
    "    print('Column types:')\n",
    "    print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712536d0",
   "metadata": {},
   "source": [
    "## 3) EDA: shape, missingness, basic time parsing and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568f95d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 88156\n",
      "Columns: 8\n",
      "Missing values (top 20):\n",
      "Synthetic_7_2kW    69199\n",
      "Synthetic_3_6kW    57046\n",
      "Flex_3_6kW         26070\n",
      "Flex_7_2kW         15532\n",
      "User_ID              390\n",
      "session_ID           390\n",
      "date_from              0\n",
      "date_to                0\n",
      "dtype: int64\n",
      "No obvious timestamp column found — inspect column names to find time info.\n"
     ]
    }
   ],
   "source": [
    "def basic_eda(df, timestamp_cols_hint=None):\n",
    "    if df.empty:\n",
    "        print('DataFrame empty — load dataset first (see previous cell).')\n",
    "        return\n",
    "\n",
    "    print('Rows:', len(df))\n",
    "    print('Columns:', len(df.columns))\n",
    "    print('Missing values (top 20):')\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "    # Try to find a timestamp column\n",
    "    ts = None\n",
    "    candidates = ['timestamp', 'start_time', 'datetime', 'date', 'time', 'usage_timestamp']\n",
    "    if timestamp_cols_hint:\n",
    "        candidates = list(timestamp_cols_hint) + candidates\n",
    "\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            ts = c\n",
    "            break\n",
    "\n",
    "    if ts is not None:\n",
    "        print('Found timestamp column:', ts)\n",
    "        try:\n",
    "            df['_ts'] = pd.to_datetime(df[ts])\n",
    "        except Exception:\n",
    "            df['_ts'] = pd.to_datetime(df[ts], errors='coerce')\n",
    "\n",
    "        print('Time span:', df['_ts'].min(), '->', df['_ts'].max())\n",
    "        df['_date'] = df['_ts'].dt.date\n",
    "        # events per day\n",
    "        daily = df.groupby('_date').size()\n",
    "        print('Days with data:', daily.shape[0])\n",
    "        print('Daily events — median:', int(daily.median()), 'mean:', int(daily.mean()))\n",
    "        plt.figure(figsize=(10,3))\n",
    "        daily.plot(title='Events per day')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No obvious timestamp column found — inspect column names to find time info.')\n",
    "\n",
    "    # Try to find an energy/charge column\n",
    "    energy_candidates = ['energy_kwh', 'energy', 'charge_kwh', 'kwh', 'energy_kWh', 'Charging_kWh']\n",
    "    energy_col = None\n",
    "    for c in energy_candidates:\n",
    "        if c in df.columns:\n",
    "            energy_col = c\n",
    "            break\n",
    "\n",
    "    if energy_col is not None:\n",
    "        print('Found energy column:', energy_col)\n",
    "        print(df[energy_col].describe())\n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.histplot(df[energy_col].dropna(), bins=50, kde=False)\n",
    "        plt.title('Energy distribution')\n",
    "        plt.show()\n",
    "\n",
    "    # If there is a user id column, show distribution per user\n",
    "    user_candidates = ['user_id', 'user', 'meter_id', 'ev_user']\n",
    "    uid = None\n",
    "    for c in user_candidates:\n",
    "        if c in df.columns:\n",
    "            uid = c\n",
    "            break\n",
    "\n",
    "    if uid is not None:\n",
    "        counts = df[uid].value_counts()\n",
    "        print('Unique users:', counts.shape[0])\n",
    "        print('Top users sample counts:')\n",
    "        display(counts.head(10))\n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.histplot(counts.values, bins=50)\n",
    "        plt.xlabel('Events per user')\n",
    "        plt.show()\n",
    "\n",
    "basic_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637a02e",
   "metadata": {},
   "source": [
    "## 4) Dataset sufficiency heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "497b5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verdict': 'good', 'n_rows': 88156, 'n_users': None, 'span_days': None, 'reasons': ['More than 20k rows — generally enough data for training a modest NN']}\n"
     ]
    }
   ],
   "source": [
    "def assess_sufficiency(df, problem='classification'):\n",
    "    # Returns a verdict and diagnostic numbers. Heuristics are intentionally conservative and\n",
    "    # intended as discussion points for the class.\n",
    "    if df.empty:\n",
    "        return {'verdict': 'no_data', 'message': 'No data available in DataFrame.'}\n",
    "\n",
    "    n = len(df)\n",
    "    users = None\n",
    "    for c in ['user_id', 'user', 'meter_id', 'ev_user']:\n",
    "        if c in df.columns:\n",
    "            users = df[c].nunique()\n",
    "            break\n",
    "\n",
    "    # time span if available\n",
    "    if '_ts' in df.columns and not df['_ts'].isna().all():\n",
    "        span_days = (df['_ts'].max() - df['_ts'].min()).days\n",
    "    else:\n",
    "        span_days = None\n",
    "\n",
    "    # simple rules of thumb (class discussion):\n",
    "    # - For a small NN on tabular data, O(10k) rows is a reasonable minimum; for complex temporal models more is needed.\n",
    "    # - If many users but few events per user, per-user modeling will be hard.\n",
    "\n",
    "    verdict = 'uncertain'\n",
    "    reasons = []\n",
    "\n",
    "    if n < 1000:\n",
    "        verdict = 'insufficient'\n",
    "        reasons.append('Less than 1k total rows — too small for a neural net from scratch')\n",
    "    elif n < 5000:\n",
    "        verdict = 'borderline'\n",
    "        reasons.append('Between 1k and 5k rows — may be possible for tiny models or heavy regularization')\n",
    "    elif n < 20000:\n",
    "        verdict = 'ok_for_small_nn'\n",
    "        reasons.append('Between 5k and 20k rows — feasible for small MLPs with careful validation')\n",
    "    else:\n",
    "        verdict = 'good'\n",
    "        reasons.append('More than 20k rows — generally enough data for training a modest NN')\n",
    "\n",
    "    if users is not None:\n",
    "        avg_per_user = n / float(users) if users>0 else None\n",
    "        if avg_per_user is not None and avg_per_user < 5:\n",
    "            reasons.append('Very few events per user — personalizing per-user models will be hard')\n",
    "\n",
    "    return {'verdict': verdict, 'n_rows': n, 'n_users': users, 'span_days': span_days, 'reasons': reasons}\n",
    "\n",
    "# Run assessment\n",
    "assess = assess_sufficiency(df)\n",
    "print(assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4168ebe",
   "metadata": {},
   "source": [
    "## 5) Baseline model (classification example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline logistic — accuracy: 0.9974398361495136\n",
      "ROC AUC: 0.9998973305954826\n"
     ]
    }
   ],
   "source": [
    "# Simplified classification baseline: use hourly `feats` only and avoid external helper functions\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "feat_path = Path('data/processed/ev_features.csv')\n",
    "if not feat_path.exists():\n",
    "    print('Features file not found:', feat_path, '— run the prep notebook first')\n",
    "else:\n",
    "    feats = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "    feats = feats.sort_index()\n",
    "    # ensure we have a next-hour numeric target 'y'\n",
    "    if 'y' not in feats.columns:\n",
    "        if 'total_energy_kWh' in feats.columns:\n",
    "            feats['y'] = feats['total_energy_kWh'].shift(-1)\n",
    "        else:\n",
    "            print('No numeric energy column found to create target; skipping classification baseline')\n",
    "            feats = None\n",
    "\n",
    "    if feats is None or 'y' not in feats.columns:\n",
    "        print('Skipping baseline — no target available')\n",
    "    else:\n",
    "        feats2 = feats.dropna(subset=['y']).copy()\n",
    "        # binary target: whether next hour has positive energy\n",
    "        feats2['next_hour_charge'] = (feats2['y'] > 0).astype(int)\n",
    "\n",
    "        # numeric features only; drop the numeric target columns if present\n",
    "        X_df = feats2.select_dtypes(include=[np.number]).copy()\n",
    "        for drop_col in ['y', 'total_energy_kWh', 'next_hour_charge']:\n",
    "            if drop_col in X_df.columns:\n",
    "                X_df = X_df.drop(columns=[drop_col])\n",
    "\n",
    "        # fill numeric NaNs with 0 (user requested behavior)\n",
    "        X_df = X_df.fillna(0)\n",
    "        X = X_df.values\n",
    "        y = feats2['next_hour_charge'].values\n",
    "\n",
    "        # Guard: need at least two classes\n",
    "        y_series = pd.Series(y)\n",
    "        if y_series.nunique() < 2:\n",
    "            print('Only one class present in the target; skipping classifier training (unique values:)', y_series.unique())\n",
    "        else:\n",
    "            # time-based split (no shuffle)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            clf = LogisticRegression(max_iter=200)\n",
    "            clf.fit(X_train, np.asarray(y_train))\n",
    "            preds = clf.predict(X_test)\n",
    "            prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "            print('Baseline logistic — accuracy:', accuracy_score(np.asarray(y_test), np.asarray(preds)))\n",
    "            if prob is not None and len(np.unique(y_test)) > 1:\n",
    "                try:\n",
    "                    print('ROC AUC:', roc_auc_score(np.asarray(y_test), np.asarray(prob)))\n",
    "                except Exception as e:\n",
    "                    print('ROC AUC calculation error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5212099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny PyTorch classifier run immediately after the baseline — uses X_train/X_test/y_train/y_test from the baseline\n",
    "# This cell is intentionally self-contained so nbconvert executes it reliably.\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if 'X_train' in globals() and 'y_train' in globals() and 'X_test' in globals() and 'y_test' in globals():\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        import torch.utils.data as data_utils\n",
    "        has_torch_local = True\n",
    "    except Exception:\n",
    "        has_torch_local = False\n",
    "\n",
    "    if not has_torch_local:\n",
    "        print('Torch not available in this kernel — skipping NN classifier run')\n",
    "    else:\n",
    "        # prepare\n",
    "        X_tr = X_train.astype(np.float32)\n",
    "        y_tr = np.asarray(y_train).astype(np.int64)\n",
    "        X_te = X_test.astype(np.float32)\n",
    "        y_te = np.asarray(y_test).astype(np.int64)\n",
    "\n",
    "        train_ds = data_utils.TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "        loader = data_utils.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "        n_features = X_tr.shape[1]\n",
    "        n_classes = int(y_tr.max()) + 1 if len(np.unique(y_tr)) > 1 else 2\n",
    "\n",
    "        class TinyClassifier(nn.Module):\n",
    "            def __init__(self, in_dim, hid=64, out_dim=2):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(in_dim, hid),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid, hid),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid, out_dim)\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "\n",
    "        device_local = torch.device('cpu')\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                device_local = torch.device('mps')\n",
    "            elif torch.cuda.is_available():\n",
    "                device_local = torch.device('cuda')\n",
    "        except Exception:\n",
    "            device_local = torch.device('cpu')\n",
    "\n",
    "        model = TinyClassifier(n_features, hid=64, out_dim=n_classes).to(device_local)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        epochs = 5\n",
    "        for epoch in range(1, epochs+1):\n",
    "            model.train()\n",
    "            running = 0.0\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device_local)\n",
    "                yb = yb.to(device_local)\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                running += float(loss.item())\n",
    "            print(f'Epoch {epoch}/{epochs} — loss {running/len(loader):.4f}')\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Xte_t = torch.from_numpy(X_te).to(device_local)\n",
    "            logits = model(Xte_t)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        try:\n",
    "            print('NN test acc:', accuracy_score(y_te, preds))\n",
    "        except Exception:\n",
    "            print('NN test predictions computed (accuracy_score unavailable or failed)')\n",
    "else:\n",
    "    print('Required training variables (X_train/y_train/X_test/y_test) not found — ensure baseline cell ran before this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312d762",
   "metadata": {},
   "source": [
    "## 6) Tiny PyTorch MLP prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea771fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — loss 0.2608\n",
      "Epoch 2/5 — loss 0.1532\n",
      "Epoch 3/5 — loss 0.1310\n",
      "Epoch 4/5 — loss 0.1202\n",
      "Epoch 5/5 — loss 0.1114\n",
      "NN test acc: 0.9974398361495136\n"
     ]
    }
   ],
   "source": [
    "# Guarded Tiny PyTorch MLP prototype\n",
    "# Only attempt to run if torch is available and training arrays are present\n",
    "if globals().get('has_torch', False) and 'X_train' in globals() and X_train is not None and 'y_train' in globals() and y_train is not None:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.utils.data as data_utils\n",
    "\n",
    "    # Convert to float32 / int64 as required\n",
    "    X_small = X_train.astype(np.float32)\n",
    "    y_small = y_train.astype(np.int64)\n",
    "\n",
    "    train_tensor = data_utils.TensorDataset(torch.from_numpy(X_small), torch.from_numpy(y_small))\n",
    "    loader = data_utils.DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
    "\n",
    "    n_features = X_small.shape[1] if X_small.ndim > 1 else 1\n",
    "    n_classes = int(y_small.max()) + 1 if len(np.unique(y_small)) > 1 else 2\n",
    "\n",
    "    class TinyMLP(nn.Module):\n",
    "        def __init__(self, in_dim, hid=64, out_dim=2):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hid),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hid, hid),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hid, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    device_local = device if device is not None else torch.device('cpu')\n",
    "    model = TinyMLP(n_features, hid=64, out_dim=n_classes).to(device_local)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device_local)\n",
    "            yb = yb.to(device_local)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += float(loss.item())\n",
    "        print(f'Epoch {epoch}/{epochs} — loss {running/len(loader):.4f}')\n",
    "\n",
    "    # quick eval on test set if available\n",
    "    model.eval()\n",
    "    if 'X_test' in globals() and X_test is not None and 'y_test' in globals() and y_test is not None:\n",
    "        with torch.no_grad():\n",
    "            X_test_t = torch.from_numpy(X_test.astype(np.float32)).to(device_local)\n",
    "            logits = model(X_test_t)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            try:\n",
    "                print('NN test acc:', accuracy_score(y_test, preds))\n",
    "            except Exception:\n",
    "                print('NN test predictions computed (accuracy_score unavailable or failed)')\n",
    "else:\n",
    "    print('Skipping PyTorch prototype (torch not available, or training data not prepared)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression MLP (self-contained) — trains a tiny PyTorch regressor on features and reports MAE/RMSE\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "feat_path = Path('data/processed/ev_features.csv')\n",
    "if not feat_path.exists():\n",
    "    print('Features file not found at', feat_path, '-- skipping regression MLP in main notebook')\n",
    "else:\n",
    "    feats = pd.read_csv(feat_path, index_col=0, parse_dates=True)\n",
    "    feats = feats.sort_index()\n",
    "    if 'y' not in feats.columns:\n",
    "        if 'total_energy_kWh' in feats.columns:\n",
    "            feats['y'] = feats['total_energy_kWh'].shift(-1)\n",
    "        else:\n",
    "            print('No numeric energy column found to create target; skipping regression MLP')\n",
    "            feats = None\n",
    "    if feats is not None:\n",
    "        feats2 = feats.dropna(subset=['y']).copy()\n",
    "        X_df = feats2.select_dtypes(include=[np.number]).copy()\n",
    "        for drop_col in ['y', 'total_energy_kWh']:\n",
    "            if drop_col in X_df.columns:\n",
    "                X_df = X_df.drop(columns=[drop_col])\n",
    "        X_df = X_df.fillna(0)\n",
    "        const_cols = [c for c in X_df.columns if X_df[c].nunique(dropna=False) <= 1]\n",
    "        if const_cols:\n",
    "            X_df = X_df.drop(columns=const_cols)\n",
    "        X = X_df.values.astype(np.float32)\n",
    "        y = feats2['y'].values.astype(np.float32)\n",
    "\n",
    "        # time-based split\n",
    "        split = int(len(X) * 0.8)\n",
    "        X_train_r, X_test_r = X[:split], X[split:]\n",
    "        y_train_r, y_test_r = y[:split], y[split:]\n",
    "        scaler = StandardScaler()\n",
    "        X_train_r = scaler.fit_transform(X_train_r)\n",
    "        X_test_r = scaler.transform(X_test_r)\n",
    "\n",
    "        # Try PyTorch\n",
    "        try:\n",
    "            import torch\n",
    "            import torch.nn as nn\n",
    "            import torch.optim as optim\n",
    "            has_torch_local = True\n",
    "        except Exception:\n",
    "            has_torch_local = False\n",
    "\n",
    "        if has_torch_local:\n",
    "            class TinyReg(nn.Module):\n",
    "                def __init__(self, in_dim, hid=64):\n",
    "                    super().__init__()\n",
    "                    self.net = nn.Sequential(nn.Linear(in_dim, hid), nn.ReLU(), nn.Linear(hid, hid), nn.ReLU(), nn.Linear(hid, 1))\n",
    "                def forward(self, x):\n",
    "                    return self.net(x).squeeze(-1)\n",
    "\n",
    "            device_local = torch.device('cpu')\n",
    "            try:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    device_local = torch.device('mps')\n",
    "                elif torch.cuda.is_available():\n",
    "                    device_local = torch.device('cuda')\n",
    "            except Exception:\n",
    "                device_local = torch.device('cpu')\n",
    "\n",
    "            model = TinyReg(X_train_r.shape[1], hid=64).to(device_local)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            epochs = 16\n",
    "            batch_size = 256\n",
    "            Xt = torch.from_numpy(X_train_r).to(device_local)\n",
    "            yt = torch.from_numpy(y_train_r).to(device_local)\n",
    "            dataset = torch.utils.data.TensorDataset(Xt, yt)\n",
    "            loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            for epoch in range(1, epochs+1):\n",
    "                model.train()\n",
    "                running = 0.0\n",
    "                for xb, yb in loader:\n",
    "                    xb = xb.to(device_local)\n",
    "                    yb = yb.to(device_local)\n",
    "                    opt.zero_grad()\n",
    "                    preds = model(xb)\n",
    "                    loss = loss_fn(preds, yb)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    running += float(loss.item())\n",
    "                print(f'Reg Epoch {epoch}/{epochs} — train loss: {running/len(loader):.6f}')\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xte = torch.from_numpy(X_test_r).to(device_local)\n",
    "                preds = model(Xte).cpu().numpy()\n",
    "            mae = mean_absolute_error(y_test_r, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(y_test_r, preds))\n",
    "            print('Regression TinyReg — MAE:', round(mae,6), 'RMSE:', round(rmse,6))\n",
    "        else:\n",
    "            print('Torch not available — falling back to LinearRegression for regression test')\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(X_train_r, y_train_r)\n",
    "            preds = lr.predict(X_test_r)\n",
    "            mae = mean_absolute_error(y_test_r, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(y_test_r, preds))\n",
    "            print('Regression LinearReg — MAE:', round(mae,6), 'RMSE:', round(rmse,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814c15f",
   "metadata": {},
   "source": [
    "## 7) Summary and next steps\n",
    "\n",
    "- The notebook computed simple heuristics (rows, users, time span) and ran a lightweight baseline and a tiny NN when possible.\n",
    "- If the dataset has >20k rows and reasonable per-user events, training a small NN from scratch is usually feasible.\n",
    "- If the dataset is smaller (<5k), prefer feature-engineered classical models, transfer learning, per-user aggregation, or data augmentation/synthetic generation.\n",
    "\n",
    "Next steps for the class project:\n",
    "1. Confirm dataset CSV availability and column names.\n",
    "2. Decide the prediction task (next-hour charging probability vs. energy amount).\n",
    "3. Create a richer feature set (time-of-day, weekday, rolling counts, weather merge).\n",
    "4. Train stronger temporal models (RNN/Transformer) only if data span and volume justify it.\n",
    "\n",
    "---\n",
    "If you want, I can now: load the actual CSV from your workspace and run the notebook cells here (if you want me to execute code), or iterate the notebook to add more focused feature engineering (e.g., time features, merge with weather)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
