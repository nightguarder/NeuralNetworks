{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1251bc4e",
   "metadata": {},
   "source": [
    "# EV Charging — Data cleanup & aggregation\n",
    "\n",
    "This notebook performs data cleaning and aggregation to produce a building-level hourly time series suitable for baseline models and neural networks. It follows the steps you suggested: load, fix dates, convert text-numbers (commas), fill missing values, aggregate to hourly totals, and ensure continuous hourly index.\n",
    "\n",
    "Save the output `df_hourly` and inspect its shape; the target is a single-column hourly series (e.g., `Flex_7_2kW`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29c074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/cyrils/Developer/Python/NeuralNetworks\n",
      "python: /Users/cyrils/Developer/Python/NeuralNetworks/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Ensure notebook runs from the repository root so relative paths work\n",
    "cwd = Path.cwd()\n",
    "repo_root = cwd\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / 'README.md').exists() or (p / '.git').exists() or (p / 'pyproject.toml').exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "if repo_root != cwd:\n",
    "    print('Changing working dir from:', cwd, 'to repo root:', repo_root)\n",
    "    os.chdir(repo_root)\n",
    "print('cwd:', Path.cwd())\n",
    "print('python:', sys.executable)\n",
    "# If you keep a .env file in the repo, optionally load it so DATASET_PATH is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77e1c5",
   "metadata": {},
   "source": [
    "## 1) Load data\n",
    "\n",
    "The notebook will try the project-relative path and fall back to the `DATASET_PATH` environment variable if set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b22489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load /Users/cyrils/Developer/Python/NeuralNetworks/data/trondheim/Dataset_2_Hourly_EV_per_user.csv\n",
      "Trying read_csv with options: {'sep': ',', 'engine': 'c', 'low_memory': False}\n",
      "read_csv failed for opts {'sep': ',', 'engine': 'c', 'low_memory': False}  —  Error tokenizing data. C error: Expected 4 fields in line 360, saw 5\n",
      "\n",
      "Trying read_csv with options: {'sep': ';', 'engine': 'c', 'low_memory': False, 'decimal': ','}\n",
      "Success with options: {'sep': ';', 'engine': 'c', 'low_memory': False, 'decimal': ','}\n",
      "Loaded rows: 88156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>User_ID</th>\n",
       "      <th>session_ID</th>\n",
       "      <th>Synthetic_3_6kW</th>\n",
       "      <th>Synthetic_7_2kW</th>\n",
       "      <th>Flex_3_6kW</th>\n",
       "      <th>Flex_7_2kW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.12.2018 10:00</td>\n",
       "      <td>21.12.2018 11:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.12.2018 10:00</td>\n",
       "      <td>21.12.2018 11:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.12.2018 11:00</td>\n",
       "      <td>21.12.2018 12:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.12.2018 12:00</td>\n",
       "      <td>21.12.2018 13:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.12.2018 13:00</td>\n",
       "      <td>21.12.2018 14:00</td>\n",
       "      <td>AdO3-4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_from           date_to User_ID  session_ID  Synthetic_3_6kW  \\\n",
       "0  21.12.2018 10:00  21.12.2018 11:00  AdO3-4         1.0             0.30   \n",
       "1  21.12.2018 10:00  21.12.2018 11:00  AdO3-4         2.0             0.87   \n",
       "2  21.12.2018 11:00  21.12.2018 12:00  AdO3-4         3.0             1.62   \n",
       "3  21.12.2018 12:00  21.12.2018 13:00  AdO3-4         3.0             3.60   \n",
       "4  21.12.2018 13:00  21.12.2018 14:00  AdO3-4         3.0             3.60   \n",
       "\n",
       "   Synthetic_7_2kW  Flex_3_6kW  Flex_7_2kW  \n",
       "0             0.30         NaN       0.060  \n",
       "1             0.87         NaN       0.114  \n",
       "2             3.24         NaN         NaN  \n",
       "3             7.20         NaN         NaN  \n",
       "4             7.20         NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['date_from', 'date_to', 'User_ID', 'session_ID', 'Synthetic_3_6kW', 'Synthetic_7_2kW', 'Flex_3_6kW', 'Flex_7_2kW']\n"
     ]
    }
   ],
   "source": [
    "# Try to find the CSV locally and load robustly (handles semicolon + decimal comma)\n",
    "candidates = [\n",
    "    'data/trondheim/Dataset_2_Hourly_EV_per_user.csv',\n",
    "    'data/trondheim/Dataset_2_Hourly_EV_per_user_sample.csv',\n",
    "    'data/trondheim/Dataset_2_Hourly_EV_per_user_*.csv',\n",
    "]\n",
    "dataset_path = None\n",
    "for p in candidates:\n",
    "    if '*' in p:\n",
    "        import glob\n",
    "        matches = glob.glob(os.path.join(os.getcwd(), p))\n",
    "        if matches:\n",
    "            dataset_path = matches[0]\n",
    "            break\n",
    "    else:\n",
    "        fp = os.path.join(os.getcwd(), p)\n",
    "        if os.path.exists(fp):\n",
    "            dataset_path = fp\n",
    "            break\n",
    "\n",
    "if dataset_path is None:\n",
    "    dataset_path = os.getenv('DATASET_PATH')\n",
    "\n",
    "# Read with several fallbacks to handle common CSV variants\n",
    "if dataset_path is None or not os.path.exists(dataset_path):\n",
    "    print('Dataset not found. Please download and place the CSV in data/trondheim/ or set DATASET_PATH.')\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    print('Attempting to load', dataset_path)\n",
    "    read_attempts = [\n",
    "        {'sep': ',', 'engine': 'c', 'low_memory': False},\n",
    "        {'sep': ';', 'engine': 'c', 'low_memory': False, 'decimal': ','},\n",
    "        {'sep': ',', 'engine': 'python', 'low_memory': False, 'on_bad_lines': 'skip'},\n",
    "        {'sep': ';', 'engine': 'python', 'low_memory': False, 'decimal': ',', 'on_bad_lines': 'skip'},\n",
    "    ]\n",
    "    last_err = None\n",
    "    for opts in read_attempts:\n",
    "        try:\n",
    "            print('Trying read_csv with options:', opts)\n",
    "            df = pd.read_csv(dataset_path, **opts)\n",
    "            print('Success with options:', opts)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print('read_csv failed for opts', opts, ' — ', e)\n",
    "    else:\n",
    "        print('All read attempts failed:', last_err)\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    if not df.empty:\n",
    "        print('Loaded rows:', len(df))\n",
    "        try:\n",
    "            display(df.head())\n",
    "        except Exception:\n",
    "            pass\n",
    "        print('Columns:', list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3292c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df exists: True\n",
      "rows,cols = (88156, 8)\n",
      "columns = ['date_from', 'date_to', 'User_ID', 'session_ID', 'Synthetic_3_6kW', 'Synthetic_7_2kW', 'Flex_3_6kW', 'Flex_7_2kW']\n"
     ]
    }
   ],
   "source": [
    "# Quick check: ensure `df` exists and show a short summary\n",
    "try:\n",
    "    print('df exists:', 'df' in globals())\n",
    "    if 'df' in globals() and not df.empty:\n",
    "        print('rows,cols =', df.shape)\n",
    "        print('columns =', list(df.columns))\n",
    "    else:\n",
    "        print('df is empty or not defined')\n",
    "except Exception as e:\n",
    "    print('Error checking df:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5f7c6",
   "metadata": {},
   "source": [
    "## 2) Fix dates\n",
    "\n",
    "Convert the primary timestamp column to pandas datetime. Update the column name if your CSV uses a different field (e.g., `date_from` vs `timestamp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb4abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using timestamp column: date_from\n",
      "Parsed timestamps — min/max: 2018-12-21 10:00:00 2020-02-01 04:00:00\n",
      "NaT count: 0\n"
     ]
    }
   ],
   "source": [
    "# Common timestamp column names to try\n",
    "ts_candidates = ['date_from', 'timestamp', 'datetime', 'start_time', 'time', 'date']\n",
    "ts_col = next((c for c in ts_candidates if c in df.columns), None)\n",
    "if ts_col is None:\n",
    "    print('No timestamp column found automatically — please set ts_col to the correct column name')\n",
    "else:\n",
    "    print('Using timestamp column:', ts_col)\n",
    "    # Try parsing with dayfirst=True (many European datasets use DD.MM.YYYY formats)\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', dayfirst=True)\n",
    "    # If parsing failed for many rows, try a strict format fallback\n",
    "    if df[ts_col].isna().sum() > 0:\n",
    "        try:\n",
    "            df[ts_col] = pd.to_datetime(df[ts_col], format='%d.%m.%Y %H:%M', errors='coerce')\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('Parsed timestamps — min/max:', df[ts_col].min(), df[ts_col].max())\n",
    "    print('NaT count:', df[ts_col].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95882595",
   "metadata": {},
   "source": [
    "## 3) Fix text numbers (commas -> dots) for energy columns\n",
    "\n",
    "Replace commas with dots and convert to numeric for energy columns. Update `energy_cols` if your file uses different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460352b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy columns found: ['Synthetic_3_6kW', 'Synthetic_7_2kW', 'Flex_3_6kW', 'Flex_7_2kW']\n",
      "After conversion — sample stats:\n",
      "Synthetic_3_6kW count    3.111000e+04\n",
      "mean     2.812241e+00\n",
      "std      1.134489e+00\n",
      "min      4.000000e-16\n",
      "25%      2.040000e+00\n",
      "50%      3.600000e+00\n",
      "75%      3.600000e+00\n",
      "max      3.600000e+00\n",
      "Name: Synthetic_3_6kW, dtype: float64\n",
      "Synthetic_7_2kW count    1.895700e+04\n",
      "mean     4.614975e+00\n",
      "std      2.464613e+00\n",
      "min      1.600000e-15\n",
      "25%      2.380000e+00\n",
      "50%      4.950000e+00\n",
      "75%      7.200000e+00\n",
      "max      7.200000e+00\n",
      "Name: Synthetic_7_2kW, dtype: float64\n",
      "Flex_3_6kW count    6.208600e+04\n",
      "mean     3.288170e+00\n",
      "std      8.331529e-01\n",
      "min      4.000000e-16\n",
      "25%      3.600000e+00\n",
      "50%      3.600000e+00\n",
      "75%      3.600000e+00\n",
      "max      3.600000e+00\n",
      "Name: Flex_3_6kW, dtype: float64\n",
      "Flex_7_2kW count    7.262400e+04\n",
      "mean     6.605624e+00\n",
      "std      1.584428e+00\n",
      "min      7.990000e-16\n",
      "25%      7.200000e+00\n",
      "50%      7.200000e+00\n",
      "75%      7.200000e+00\n",
      "max      7.200000e+00\n",
      "Name: Flex_7_2kW, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Edit this list to match your CSV column names if needed\n",
    "energy_cols = ['Synthetic_3_6kW', 'Synthetic_7_2kW', 'Flex_3_6kW', 'Flex_7_2kW']\n",
    "energy_cols = [c for c in energy_cols if c in df.columns]\n",
    "print('Energy columns found:', energy_cols)\n",
    "\n",
    "for col in energy_cols:\n",
    "    # coerce to string, replace comma with dot, then numeric\n",
    "    df[col] = df[col].astype(str).str.replace(',', '.')\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print('After conversion — sample stats:')\n",
    "for col in energy_cols:\n",
    "    print(col, df[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2783a52",
   "metadata": {},
   "source": [
    "## 4) Handle missing data\n",
    "\n",
    "Assumption: NaN means zero energy for that slot (common for sparse charging datasets). Change strategy if your data semantics differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c90d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled NaNs with 0 for energy columns\n"
     ]
    }
   ],
   "source": [
    "if len(energy_cols) > 0 and not df.empty:\n",
    "    df[energy_cols] = df[energy_cols].fillna(0)\n",
    "    print('Filled NaNs with 0 for energy columns')\n",
    "else:\n",
    "    print('No energy columns found or empty dataframe — skipping fillna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783b1a1",
   "metadata": {},
   "source": [
    "## 5) Aggregate to building-level hourly series\n",
    "\n",
    "Select one target variable (e.g., `Flex_7_2kW`) to forecast. We'll sum across rows for each timestamp and resample hourly to ensure continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8bcfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape for NN (rows,cols): (9763, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/bn76z7n539g1w32jqmbq83q80000gn/T/ipykernel_24700/1599696722.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df_hourly = df_grouped.set_index(ts_col).resample('H').sum().fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_energy_kWh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_from</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-21 10:00:00</th>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21 11:00:00</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21 12:00:00</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21 13:00:00</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21 14:00:00</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     total_energy_kWh\n",
       "date_from                            \n",
       "2018-12-21 10:00:00             0.174\n",
       "2018-12-21 11:00:00             0.000\n",
       "2018-12-21 12:00:00             0.000\n",
       "2018-12-21 13:00:00             0.000\n",
       "2018-12-21 14:00:00             0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose target column\n",
    "target_col = 'Flex_7_2kW' if 'Flex_7_2kW' in df.columns else (energy_cols[0] if len(energy_cols)>0 else None)\n",
    "if target_col is None or df.empty:\n",
    "    print('No target column available — cannot create hourly series')\n",
    "    df_hourly = pd.DataFrame()\n",
    "else:\n",
    "    # Group by timestamp and sum the energy across rows (building-level)\n",
    "    df_tmp = df[[ts_col, target_col]].copy()\n",
    "    df_tmp = df_tmp.dropna(subset=[ts_col])\n",
    "    df_grouped = df_tmp.groupby(ts_col)[target_col].sum().reset_index()\n",
    "    # set index and resample hourly (fill missing hours with 0)\n",
    "    df_hourly = df_grouped.set_index(ts_col).resample('H').sum().fillna(0)\n",
    "    df_hourly = df_hourly.rename(columns={target_col: 'total_energy_kWh'})\n",
    "    print('Final Shape for NN (rows,cols):', df_hourly.shape)\n",
    "    display(df_hourly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f0ade",
   "metadata": {},
   "source": [
    "## 6) Save cleaned result (optional)\n",
    "\n",
    "Save `df_hourly` to `data/processed/ev_hourly.csv` for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0869ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed hourly CSV to data/processed/ev_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path('data/processed')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / 'ev_hourly.csv'\n",
    "if not df_hourly.empty:\n",
    "    df_hourly.to_csv(out_path)\n",
    "    print('Saved processed hourly CSV to', out_path)\n",
    "else:\n",
    "    print('No hourly data to save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a308ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to data/processed/ev_features.csv shape= (9762, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/bn76z7n539g1w32jqmbq83q80000gn/T/ipykernel_24700/2859571553.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Merge hourly weather into EV hourly data, create time/lags/rolling features, and save features CSV\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ev_path = Path('data/processed/ev_hourly.csv')\n",
    "weather_path = Path('data/trondheim/Norway_Trondheim_ExactLoc_Weather.csv')\n",
    "out_path = Path('data/processed/ev_features.csv')\n",
    "\n",
    "assert ev_path.exists(), 'Run earlier cells to produce ev_hourly.csv'\n",
    "ev = pd.read_csv(ev_path, index_col=0, parse_dates=True).sort_index()\n",
    "\n",
    "# Load weather (daily); try to parse known datetime column names\n",
    "if weather_path.exists():\n",
    "    w = pd.read_csv(weather_path, sep=None, engine='python')\n",
    "    dt_cols = [c for c in w.columns if 'date' in c.lower() or 'time' in c.lower() or 'datetime' in c.lower()]\n",
    "    if len(dt_cols):\n",
    "        dtc = dt_cols[0]\n",
    "        w[dtc] = pd.to_datetime(w[dtc], dayfirst=True, errors='coerce')\n",
    "        w = w.set_index(dtc).sort_index()\n",
    "        # Collapse duplicate timestamps by averaging numeric columns, then resample/interpolate to hourly\n",
    "        w_num = w.select_dtypes(include=[float, int, 'number'])\n",
    "        if not w_num.empty:\n",
    "            w_num = w_num.groupby(w_num.index).mean()\n",
    "            w_hour = w_num.resample('h').interpolate(method='time').ffill().bfill()\n",
    "            df = ev.join(w_hour, how='left')\n",
    "            df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        else:\n",
    "            # no numeric weather columns to merge\n",
    "            print('Weather had no numeric columns; skipping numeric merge')\n",
    "            df = ev.copy()\n",
    "    else:\n",
    "        print('No datetime-like column found in weather file; skipping weather merge')\n",
    "        df = ev.copy()\n",
    "else:\n",
    "    print('Weather file not found; creating features from EV only')\n",
    "    df = ev.copy()\n",
    "\n",
    "# Feature engineering: time features\n",
    "if 'total_energy_kWh' not in df.columns:\n",
    "    df['total_energy_kWh'] = ev['total_energy_kWh']\n",
    "\n",
    "df['hour'] = df.index.hour\n",
    "df['dow'] = df.index.dayofweek\n",
    "# cyclic encoding for hour and day-of-week\n",
    "\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\n",
    "\n",
    "# Lags and rolling stats\n",
    "for lag in [1, 24, 168]:\n",
    "    df[f'lag_{lag}'] = df['total_energy_kWh'].shift(lag)\n",
    "\n",
    "df['roll_3h'] = df['total_energy_kWh'].rolling(3, min_periods=1).mean()\n",
    "df['roll_24h'] = df['total_energy_kWh'].rolling(24, min_periods=1).mean()\n",
    "\n",
    "# Drop initial rows with NaN lags if any\n",
    "df = df.dropna(subset=['lag_1'])\n",
    "\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_path)\n",
    "print('Saved features to', out_path, 'shape=', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b0c68",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "\n",
    "- Check column names in the raw CSV and edit `ts_candidates` and `energy_cols` accordingly.\n",
    "- If NaN semantics differ, replace `fillna(0)` with a strategy appropriate for your data.\n",
    "- You can now use `notebooks/ev_charging_analysis.ipynb` to load `data/processed/ev_hourly.csv` and continue with EDA and modelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
